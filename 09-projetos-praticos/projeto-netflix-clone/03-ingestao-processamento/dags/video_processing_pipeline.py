"""
Netflix Clone - Video Processing Pipeline DAG
==============================================

Complete video processing pipeline from ingestion to CDN deployment.

Pipeline Steps:
1. Validate source video
2. Extract metadata
3. Transcode to multiple resolutions (parallel)
4. Generate HLS manifests
5. Apply DRM encryption
6. Quality validation (VMAF)
7. Generate thumbnails
8. Upload to S3/CDN
9. Update database
10. Send notifications

Schedule: Triggered by S3 upload event via Lambda/EventBridge
"""

from airflow import DAG
from airflow.decorators import task, task_group
from airflow.operators.python import PythonOperator, BranchPythonOperator
from airflow.operators.empty import EmptyOperator
from airflow.providers.amazon.aws.operators.s3 import S3CopyObjectOperator, S3DeleteObjectsOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.providers.http.operators.http import SimpleHttpOperator
from airflow.utils.task_group import TaskGroup
from airflow.models import Variable
from airflow.exceptions import AirflowException

from datetime import datetime, timedelta
from typing import Dict, List, Optional
import json
import logging
import subprocess
import os
from pathlib import Path

logger = logging.getLogger(__name__)

# Default args
default_args = {
    'owner': 'data-engineering',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email': ['data-team@netflix-clone.com'],
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5),
    'execution_timeout': timedelta(hours=6),  # Max 6 hours for large files
}

# Configuration from Airflow Variables
S3_RAW_BUCKET = Variable.get("s3_raw_content_bucket", "netflix-raw-content")
S3_ENCODED_BUCKET = Variable.get("s3_encoded_content_bucket", "netflix-encoded-content")
S3_CDN_BUCKET = Variable.get("s3_cdn_bucket", "netflix-cdn-content")
POSTGRES_CONN_ID = "netflix_postgres"
SLACK_WEBHOOK_URL = Variable.get("slack_webhook_url", "")


@task
def validate_source_video(s3_key: str) -> Dict:
    """
    Task 1: Validate source video file

    Checks:
    - File exists and is readable
    - Valid video codec
    - Minimum resolution (720p)
    - Has audio track
    - No corruption
    """
    logger.info(f"Validating source video: {s3_key}")

    # Download file from S3
    s3_hook = S3Hook()
    local_file = f"/tmp/{Path(s3_key).name}"

    s3_hook.download_file(
        key=s3_key,
        bucket_name=S3_RAW_BUCKET,
        local_path=local_file
    )

    # Run ffprobe to validate
    cmd = [
        'ffprobe',
        '-v', 'error',
        '-show_format',
        '-show_streams',
        '-print_format', 'json',
        local_file
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        data = json.loads(result.stdout)

        # Find video and audio streams
        video_stream = next(
            (s for s in data['streams'] if s['codec_type'] == 'video'),
            None
        )
        audio_stream = next(
            (s for s in data['streams'] if s['codec_type'] == 'audio'),
            None
        )

        if not video_stream:
            raise AirflowException("No video stream found in file")

        if not audio_stream:
            raise AirflowException("No audio stream found in file")

        width = int(video_stream['width'])
        height = int(video_stream['height'])

        # Minimum resolution check
        if height < 720:
            raise AirflowException(f"Resolution too low: {width}x{height}. Minimum: 1280x720")

        # Extract key metadata
        metadata = {
            'valid': True,
            'duration': float(data['format'].get('duration', 0)),
            'size': int(data['format'].get('size', 0)),
            'bitrate': int(data['format'].get('bit_rate', 0)),
            'video': {
                'codec': video_stream['codec_name'],
                'width': width,
                'height': height,
                'fps': eval(video_stream.get('r_frame_rate', '0/1')),
                'bitrate': int(video_stream.get('bit_rate', 0)) if 'bit_rate' in video_stream else 0,
            },
            'audio': {
                'codec': audio_stream['codec_name'],
                'sample_rate': int(audio_stream['sample_rate']),
                'channels': int(audio_stream['channels']),
            },
            's3_key': s3_key,
            'local_file': local_file
        }

        logger.info(f"Video validation passed: {width}x{height}, {metadata['duration']:.1f}s")

        return metadata

    except subprocess.CalledProcessError as e:
        raise AirflowException(f"ffprobe failed: {e.stderr}")
    except Exception as e:
        raise AirflowException(f"Validation failed: {str(e)}")


@task
def extract_metadata(validation_result: Dict) -> Dict:
    """
    Task 2: Extract detailed metadata

    - Frame-level analysis
    - Scene detection
    - Color profile
    - Audio fingerprint
    """
    logger.info("Extracting detailed metadata")

    local_file = validation_result['local_file']

    # Extract first frame as poster
    poster_path = f"/tmp/poster_{Path(local_file).stem}.jpg"

    cmd = [
        'ffmpeg',
        '-i', local_file,
        '-ss', '00:00:05',  # 5 seconds in (skip intro)
        '-vframes', '1',
        '-q:v', '2',  # High quality JPEG
        poster_path,
        '-y'
    ]

    subprocess.run(cmd, capture_output=True, check=True)

    # Upload poster to S3
    s3_hook = S3Hook()
    poster_s3_key = f"posters/{Path(local_file).stem}.jpg"

    s3_hook.load_file(
        filename=poster_path,
        key=poster_s3_key,
        bucket_name=S3_ENCODED_BUCKET,
        replace=True
    )

    metadata = {
        **validation_result,
        'poster_s3_key': poster_s3_key,
        'extraction_complete': True
    }

    logger.info(f"Metadata extracted, poster uploaded: {poster_s3_key}")

    return metadata


@task_group(group_id='transcode_video')
def transcode_video_group(metadata: Dict):
    """
    Task 3: Transcode video to multiple resolutions (parallel)

    Profiles: 4K, 1080p, 720p, 480p, 360p
    """

    @task
    def transcode_4k(metadata: Dict) -> Dict:
        """Transcode to 4K (3840x2160)"""
        return _transcode_profile(metadata, '4k', 3840, 2160, '15000k', 'libx265')

    @task
    def transcode_1080p(metadata: Dict) -> Dict:
        """Transcode to 1080p (1920x1080)"""
        return _transcode_profile(metadata, '1080p', 1920, 1080, '5000k', 'libx264')

    @task
    def transcode_720p(metadata: Dict) -> Dict:
        """Transcode to 720p (1280x720)"""
        return _transcode_profile(metadata, '720p', 1280, 720, '2500k', 'libx264')

    @task
    def transcode_480p(metadata: Dict) -> Dict:
        """Transcode to 480p (854x480)"""
        return _transcode_profile(metadata, '480p', 854, 480, '1000k', 'libx264')

    @task
    def transcode_360p(metadata: Dict) -> Dict:
        """Transcode to 360p (640x360)"""
        return _transcode_profile(metadata, '360p', 640, 360, '500k', 'libx264')

    # Only transcode 4K if source is >= 4K
    source_height = metadata['video']['height']

    profiles = []
    if source_height >= 2160:
        profiles.append(transcode_4k(metadata))
    if source_height >= 1080:
        profiles.append(transcode_1080p(metadata))
    if source_height >= 720:
        profiles.append(transcode_720p(metadata))

    # Always include 480p and 360p
    profiles.append(transcode_480p(metadata))
    profiles.append(transcode_360p(metadata))

    return profiles


def _transcode_profile(metadata: Dict, profile_name: str,
                       width: int, height: int,
                       bitrate: str, codec: str) -> Dict:
    """
    Helper function to transcode a single profile
    """
    logger.info(f"Transcoding {profile_name}: {width}x{height} @ {bitrate}")

    local_file = metadata['local_file']
    content_id = Path(local_file).stem

    # Output directory
    output_dir = f"/tmp/{content_id}/{profile_name}"
    os.makedirs(output_dir, exist_ok=True)

    output_m3u8 = f"{output_dir}/output.m3u8"
    segment_pattern = f"{output_dir}/segment_%05d.ts"

    # FFmpeg command
    cmd = [
        'ffmpeg',
        '-i', local_file,
        '-c:v', codec,
        '-preset', 'medium',
        '-b:v', bitrate,
        '-maxrate', str(int(bitrate.replace('k', '')) * 1.07) + 'k',
        '-bufsize', str(int(bitrate.replace('k', '')) * 1.5) + 'k',
        '-vf', f'scale={width}:{height}',
        '-g', '48',  # GOP size
        '-sc_threshold', '0',
        '-c:a', 'aac',
        '-b:a', '128k',
        '-ar', '48000',
        '-ac', '2',
        '-f', 'hls',
        '-hls_time', '6',
        '-hls_playlist_type', 'vod',
        '-hls_segment_filename', segment_pattern,
        output_m3u8,
        '-y'
    ]

    try:
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)

        # Upload segments to S3
        s3_hook = S3Hook()
        s3_prefix = f"content/{content_id}/{profile_name}/"

        for file in Path(output_dir).glob('*'):
            if file.is_file():
                s3_key = f"{s3_prefix}{file.name}"
                s3_hook.load_file(
                    filename=str(file),
                    key=s3_key,
                    bucket_name=S3_ENCODED_BUCKET,
                    replace=True
                )

        logger.info(f"âœ“ {profile_name} transcoding complete, uploaded to S3")

        return {
            'profile': profile_name,
            'success': True,
            's3_prefix': s3_prefix,
            'segment_count': len(list(Path(output_dir).glob('segment_*.ts')))
        }

    except subprocess.CalledProcessError as e:
        logger.error(f"âœ— {profile_name} transcoding failed: {e.stderr}")
        raise AirflowException(f"Transcoding failed for {profile_name}")


@task
def generate_master_playlist(metadata: Dict, transcode_results: List[Dict]) -> str:
    """
    Task 4: Generate HLS master playlist

    Combines all variants into a single master.m3u8
    """
    logger.info("Generating HLS master playlist")

    content_id = Path(metadata['local_file']).stem
    master_playlist = f"/tmp/{content_id}/master.m3u8"

    with open(master_playlist, 'w') as f:
        f.write('#EXTM3U\n')
        f.write('#EXT-X-VERSION:3\n\n')

        # Define bandwidth for each profile
        bandwidth_map = {
            '4k': 15000000,
            '1080p': 5000000,
            '720p': 2500000,
            '480p': 1000000,
            '360p': 500000
        }

        resolution_map = {
            '4k': '3840x2160',
            '1080p': '1920x1080',
            '720p': '1280x720',
            '480p': '854x480',
            '360p': '640x360'
        }

        for result in transcode_results:
            if result['success']:
                profile = result['profile']
                f.write(f'#EXT-X-STREAM-INF:BANDWIDTH={bandwidth_map[profile]},'
                       f'RESOLUTION={resolution_map[profile]}\n')
                f.write(f'{profile}/output.m3u8\n\n')

    # Upload to S3
    s3_hook = S3Hook()
    s3_key = f"content/{content_id}/master.m3u8"

    s3_hook.load_file(
        filename=master_playlist,
        key=s3_key,
        bucket_name=S3_ENCODED_BUCKET,
        replace=True
    )

    logger.info(f"Master playlist uploaded: {s3_key}")

    return s3_key


@task
def apply_drm(metadata: Dict, master_playlist_s3_key: str) -> Dict:
    """
    Task 5: Apply DRM encryption

    - Widevine (Android, Chrome)
    - FairPlay (iOS, Safari)
    - PlayReady (Windows)

    Note: This is a placeholder. Real DRM requires licensed services.
    """
    logger.info("Applying DRM encryption")

    # In production, integrate with:
    # - Google Widevine Cloud DRM
    # - Apple FairPlay Streaming
    # - Microsoft PlayReady

    content_id = Path(metadata['local_file']).stem

    drm_config = {
        'content_id': content_id,
        'widevine': {
            'enabled': True,
            'license_url': f'https://drm.netflix-clone.com/widevine/license?content_id={content_id}'
        },
        'fairplay': {
            'enabled': True,
            'certificate_url': f'https://drm.netflix-clone.com/fairplay/cert',
            'license_url': f'https://drm.netflix-clone.com/fairplay/license?content_id={content_id}'
        },
        'playready': {
            'enabled': True,
            'license_url': f'https://drm.netflix-clone.com/playready/license?content_id={content_id}'
        }
    }

    # Store DRM config
    drm_config_file = f"/tmp/{content_id}/drm_config.json"
    with open(drm_config_file, 'w') as f:
        json.dump(drm_config, f, indent=2)

    # Upload to S3
    s3_hook = S3Hook()
    s3_key = f"content/{content_id}/drm_config.json"

    s3_hook.load_file(
        filename=drm_config_file,
        key=s3_key,
        bucket_name=S3_ENCODED_BUCKET,
        replace=True
    )

    logger.info("DRM configuration saved")

    return drm_config


@task
def quality_validation(metadata: Dict, transcode_results: List[Dict]) -> Dict:
    """
    Task 6: Quality validation

    - VMAF scoring (target: >85)
    - Playback testing
    - Audio sync check
    """
    logger.info("Running quality validation")

    # For each transcoded profile, calculate VMAF score
    # VMAF: Netflix's perceptual video quality metric (0-100)

    validation_results = {
        'passed': True,
        'profiles': []
    }

    for result in transcode_results:
        if not result['success']:
            continue

        profile = result['profile']

        # Placeholder: In production, calculate actual VMAF
        # This requires comparing encoded video to source

        # Simulated VMAF score (in reality, run ffmpeg libvmaf filter)
        vmaf_score = 90.0  # Assume good quality

        passed = vmaf_score >= 85.0

        validation_results['profiles'].append({
            'profile': profile,
            'vmaf_score': vmaf_score,
            'passed': passed
        })

        if not passed:
            validation_results['passed'] = False
            logger.warning(f"Quality check failed for {profile}: VMAF={vmaf_score}")
        else:
            logger.info(f"âœ“ {profile} quality check passed: VMAF={vmaf_score}")

    if not validation_results['passed']:
        raise AirflowException("Quality validation failed for one or more profiles")

    return validation_results


@task
def generate_thumbnails(metadata: Dict) -> List[str]:
    """
    Task 7: Generate thumbnails

    - Scene detection
    - Extract candidate thumbnails
    - ML scoring for aesthetics
    - Select top 5
    """
    logger.info("Generating thumbnails")

    local_file = metadata['local_file']
    content_id = Path(local_file).stem
    duration = metadata['duration']

    # Extract thumbnails at key moments
    # Strategy: Extract at 10%, 25%, 50%, 75%, 90% of duration
    timestamps = [duration * pct for pct in [0.1, 0.25, 0.5, 0.75, 0.9]]

    thumbnail_s3_keys = []
    s3_hook = S3Hook()

    for i, timestamp in enumerate(timestamps):
        thumbnail_path = f"/tmp/{content_id}_thumb_{i}.jpg"

        cmd = [
            'ffmpeg',
            '-ss', str(timestamp),
            '-i', local_file,
            '-vframes', '1',
            '-q:v', '2',
            '-vf', 'scale=640:360',  # Thumbnail size
            thumbnail_path,
            '-y'
        ]

        subprocess.run(cmd, capture_output=True, check=True)

        # Upload to S3
        s3_key = f"thumbnails/{content_id}/thumb_{i}.jpg"
        s3_hook.load_file(
            filename=thumbnail_path,
            key=s3_key,
            bucket_name=S3_CDN_BUCKET,
            replace=True
        )

        thumbnail_s3_keys.append(s3_key)

    logger.info(f"Generated {len(thumbnail_s3_keys)} thumbnails")

    return thumbnail_s3_keys


@task
def update_database(metadata: Dict, master_playlist_s3_key: str,
                    drm_config: Dict, thumbnails: List[str]) -> int:
    """
    Task 8: Update PostgreSQL database

    - Insert content record
    - Create video_assets for each resolution
    - Link thumbnails
    - Set status to 'available'
    """
    logger.info("Updating database")

    pg_hook = PostgresHook(postgres_conn_id=POSTGRES_CONN_ID)
    conn = pg_hook.get_conn()
    cursor = conn.cursor()

    content_id = Path(metadata['local_file']).stem

    try:
        # Insert content
        cursor.execute("""
            INSERT INTO content (
                title, content_type, duration_seconds,
                release_date, status, created_at
            ) VALUES (
                %s, %s, %s, %s, %s, NOW()
            )
            RETURNING content_id
        """, (
            content_id,  # Placeholder: use actual title
            'movie',
            int(metadata['duration']),
            datetime.now().date(),
            'available'
        ))

        content_id_db = cursor.fetchone()[0]

        # Insert video assets for each resolution
        for profile in ['4k', '1080p', '720p', '480p', '360p']:
            cursor.execute("""
                INSERT INTO video_assets (
                    content_id, resolution, bitrate_kbps,
                    codec, file_path, file_size_mb, created_at
                ) VALUES (
                    %s, %s, %s, %s, %s, %s, NOW()
                )
            """, (
                content_id_db,
                profile,
                5000,  # Placeholder
                'h264',
                f"s3://{S3_ENCODED_BUCKET}/content/{content_id}/{profile}/",
                1000.0,  # Placeholder
            ))

        # Link thumbnails
        for i, thumb_s3_key in enumerate(thumbnails):
            cursor.execute("""
                INSERT INTO content_metadata (
                    content_id, metadata_key, metadata_value
                ) VALUES (
                    %s, %s, %s
                )
            """, (
                content_id_db,
                f'thumbnail_{i}',
                f"https://cdn.netflix-clone.com/{thumb_s3_key}"
            ))

        conn.commit()
        logger.info(f"Database updated: content_id={content_id_db}")

        return content_id_db

    except Exception as e:
        conn.rollback()
        raise AirflowException(f"Database update failed: {str(e)}")
    finally:
        cursor.close()
        conn.close()


@task
def send_notification(metadata: Dict, content_id: int):
    """
    Task 9: Send notification

    - Slack message
    - Update dashboard
    - Alert content team
    """
    logger.info("Sending notification")

    content_title = Path(metadata['local_file']).stem

    message = {
        "text": f"âœ“ Video processing complete!",
        "blocks": [
            {
                "type": "header",
                "text": {
                    "type": "plain_text",
                    "text": "ðŸŽ¬ Video Processing Complete"
                }
            },
            {
                "type": "section",
                "fields": [
                    {
                        "type": "mrkdwn",
                        "text": f"*Content:*\n{content_title}"
                    },
                    {
                        "type": "mrkdwn",
                        "text": f"*Content ID:*\n{content_id}"
                    },
                    {
                        "type": "mrkdwn",
                        "text": f"*Duration:*\n{metadata['duration']:.0f}s"
                    },
                    {
                        "type": "mrkdwn",
                        "text": f"*Resolution:*\n{metadata['video']['width']}x{metadata['video']['height']}"
                    }
                ]
            }
        ]
    }

    # Send to Slack (if configured)
    if SLACK_WEBHOOK_URL:
        import requests
        requests.post(SLACK_WEBHOOK_URL, json=message)

    logger.info(f"Notification sent for content_id={content_id}")


# Define the DAG
with DAG(
    'video_processing_pipeline',
    default_args=default_args,
    description='Complete video processing pipeline for streaming platform',
    schedule_interval=None,  # Triggered by events
    catchup=False,
    max_active_runs=10,  # Allow parallel processing of multiple videos
    tags=['video', 'processing', 'netflix'],
) as dag:

    # Task 1: Validate
    validation = validate_source_video(
        s3_key="{{ dag_run.conf['s3_key'] }}"
    )

    # Task 2: Extract metadata
    metadata_extracted = extract_metadata(validation)

    # Task 3: Transcode (parallel)
    transcoded = transcode_video_group(metadata_extracted)

    # Task 4: Master playlist
    master_playlist = generate_master_playlist(metadata_extracted, transcoded)

    # Task 5: DRM
    drm = apply_drm(metadata_extracted, master_playlist)

    # Task 6: Quality validation
    quality = quality_validation(metadata_extracted, transcoded)

    # Task 7: Thumbnails
    thumbnails = generate_thumbnails(metadata_extracted)

    # Task 8: Update database
    db_updated = update_database(
        metadata_extracted,
        master_playlist,
        drm,
        thumbnails
    )

    # Task 9: Notification
    notification = send_notification(metadata_extracted, db_updated)

    # Define dependencies
    validation >> metadata_extracted >> transcoded >> master_playlist
    master_playlist >> drm >> quality >> thumbnails >> db_updated >> notification


# Usage:
# Trigger via:
# airflow dags trigger video_processing_pipeline \
#   --conf '{"s3_key": "raw-content/movie_2024_01_01.mp4"}'
