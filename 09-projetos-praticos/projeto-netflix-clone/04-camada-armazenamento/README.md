# ðŸ’¾ Camada de Armazenamento

## ðŸ“‹ VisÃ£o Geral

Sistema unificado de armazenamento multi-cloud com otimizaÃ§Ã£o de custos para plataforma de streaming Netflix.

**Capacidade:**
- Petabytes de conteÃºdo de vÃ­deo
- Multi-cloud (AWS S3, Google Cloud Storage)
- CDN integration (CloudFront, Cloud CDN)
- Lifecycle policies automÃ¡ticas
- Savings: 65-70% em storage costs

**Storage Tiers:**
- **Hot** (0-30 dias): Standard - acesso frequente
- **Warm** (30-90 dias): Infrequent Access - acesso ocasional
- **Cold** (90-180 dias): Glacier Instant Retrieval - acesso raro
- **Archive** (>180 dias): Glacier/Deep Archive - compliance

---

## ðŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STORAGE LAYER                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Upload Pipeline                                 â”‚
â”‚                                                               â”‚
â”‚  Content Team â†’ Storage Manager â†’ S3/GCS â†’ CDN              â”‚
â”‚                      â”‚                                        â”‚
â”‚                      â”œâ”€ Multipart upload (>100MB)           â”‚
â”‚                      â”œâ”€ Metadata tagging                     â”‚
â”‚                      â”œâ”€ Encryption (AES-256)                â”‚
â”‚                      â”œâ”€ Cache control headers               â”‚
â”‚                      â””â”€ CDN invalidation                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Storage Buckets                                 â”‚
â”‚                                                               â”‚
â”‚  S3/GCS Raw Content                                          â”‚
â”‚  â”œâ”€ raw-content/               (Source files)                â”‚
â”‚  â”‚   â””â”€ Lifecycle: Delete after 30 days                     â”‚
â”‚                                                               â”‚
â”‚  S3/GCS Encoded Content                                      â”‚
â”‚  â”œâ”€ content/                   (Transcoded videos)           â”‚
â”‚  â”‚   â”œâ”€ 0-30 days:   STANDARD                               â”‚
â”‚  â”‚   â”œâ”€ 30-90 days:  STANDARD_IA (46% cheaper)             â”‚
â”‚  â”‚   â”œâ”€ 90-180 days: GLACIER_IR (83% cheaper)              â”‚
â”‚  â”‚   â””â”€ >180 days:   GLACIER (84% cheaper)                 â”‚
â”‚  â”‚                                                           â”‚
â”‚  â”œâ”€ thumbnails/                (Images)                      â”‚
â”‚  â”‚   â””â”€ Lifecycle: IA after 90 days                         â”‚
â”‚  â”‚                                                           â”‚
â”‚  â””â”€ logs/                      (Access logs)                 â”‚
â”‚      â”œâ”€ 0-30 days:   STANDARD                               â”‚
â”‚      â”œâ”€ 30-365 days: GLACIER_IR                             â”‚
â”‚      â””â”€ >365 days:   DEEP_ARCHIVE                           â”‚
â”‚                                                               â”‚
â”‚  S3/GCS CDN Content                                          â”‚
â”‚  â””â”€ Public bucket for CloudFront/Cloud CDN origin           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CDN Distribution                                â”‚
â”‚                                                               â”‚
â”‚  CloudFront / Cloud CDN                                      â”‚
â”‚  â”œâ”€ Edge locations worldwide (200+ POPs)                    â”‚
â”‚  â”œâ”€ Cache hit ratio: >95%                                    â”‚
â”‚  â”œâ”€ TTL: 30 days for segments, 1 min for manifests          â”‚
â”‚  â””â”€ Savings: 80% reduction in origin requests               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ðŸ“¦ Componentes

### 1. `storage_manager.py`

Gerenciador unificado de storage multi-cloud.

**Features:**
- AbstraÃ§Ã£o multi-cloud (S3, GCS)
- Upload/download com multipart (files >100MB)
- Parallel directory upload
- Presigned URLs para acesso temporÃ¡rio
- Metadata management
- Checksum verification
- CDN cache invalidation

**Providers Suportados:**
- **AWS S3** - Amazon Simple Storage Service
- **Google Cloud Storage** - GCS

**Usage:**

```python
from storage_manager import StorageManager, StorageConfig

# Configure S3
config = StorageConfig(
    provider='s3',
    bucket_name='netflix-content-prod',
    region='us-east-1',
    cdn_domain='d123456.cloudfront.net',
    cdn_distribution_id='E1234567890ABC',
    enable_encryption=True
)

storage = StorageManager(config)

# Upload single file
result = storage.upload(
    local_path='movie.mp4',
    key='content/movie_123/1080p/video.mp4',
    metadata={'content_id': '123', 'resolution': '1080p'}
)

print(f"CDN URL: {result.cdn_url}")

# Upload entire directory (parallel)
results = storage.upload_directory(
    local_dir='output/movie_123',
    prefix='content/movie_123',
    parallel=True,
    max_workers=10
)

# Get temporary download URL (expires in 1 hour)
url = storage.get_url(
    key='content/movie_123/master.m3u8',
    temporary=True,
    expires_in=3600
)

# List files
files = storage.list(prefix='content/movie_123/')

# Delete old content
storage.delete_prefix('content/old_movie/')

# Verify upload integrity
verified = storage.verify_upload(
    local_path='movie.mp4',
    key='content/movie_123/video.mp4'
)
```

**Google Cloud Storage:**

```python
# Configure GCS
config = StorageConfig(
    provider='gcs',
    bucket_name='netflix-content-prod',
    cdn_domain='cdn.netflix-clone.com'
)

storage = StorageManager(config)

# API is identical to S3
storage.upload('video.mp4', 'content/123/video.mp4')
```

### 2. `lifecycle_policies.py`

Gerenciamento automÃ¡tico de lifecycle policies para otimizaÃ§Ã£o de custos.

**Features:**
- Automated storage tiering
- Cost analysis e savings calculation
- S3 e GCS support
- Custom rules

**S3 Lifecycle Rules:**

```python
from lifecycle_policies import S3LifecycleManager

lifecycle = S3LifecycleManager(
    bucket_name='netflix-content-prod',
    region='us-east-1'
)

# Create default content policy
lifecycle.create_content_lifecycle_policy()

# Analyze costs
cost_analysis = lifecycle.analyze_storage_costs()

print(f"Total size: {cost_analysis['total_size_gb']:.2f} GB")
print(f"Monthly cost: ${cost_analysis['total_monthly_cost']:.2f}")
print(f"Savings: ${cost_analysis['monthly_savings']:.2f}")
print(f"Savings %: {cost_analysis['savings_percentage']:.1f}%")
```

**Lifecycle Rules Criadas:**

| Prefix | Age | Action | Savings |
|--------|-----|--------|---------|
| `content/` | 30 days | â†’ STANDARD_IA | 46% |
| `content/` | 90 days | â†’ GLACIER_IR | 83% |
| `content/` | 180 days | â†’ GLACIER | 84% |
| `raw-content/` | 30 days | DELETE | 100% |
| `logs/` | 30 days | â†’ GLACIER_IR | 83% |
| `logs/` | 365 days | â†’ DEEP_ARCHIVE | 96% |
| `temp/` | 7 days | DELETE | 100% |

**Cost Calculator:**

```python
from lifecycle_policies import calculate_savings

# Calculate savings for 1 PB content library
savings = calculate_savings(
    total_size_gb=1_000_000,  # 1 PB
    content_age_distribution={
        'hot': 0.15,      # 15% new releases (0-30 days)
        'warm': 0.25,     # 25% recent (30-90 days)
        'cold': 0.35,     # 35% older (90-180 days)
        'archive': 0.25   # 25% catalog (>180 days)
    }
)

print(f"Monthly savings: ${savings['monthly_savings']:,.2f}")
print(f"Annual savings: ${savings['annual_savings']:,.2f}")
```

**Example Output:**

```
SAVINGS CALCULATOR - 1 PB Content Library
============================================================
Total Size:              1,000,000 GB (1 PB)
Cost without lifecycle:  $23,000.00/month
Cost with lifecycle:     $7,897.50/month
Monthly Savings:         $15,102.50
Annual Savings:          $181,230.00
Savings Percentage:      65.7%

Breakdown:
  Hot        150,000 GB â†’ $  3,450.00
  Warm       250,000 GB â†’ $  3,125.00
  Cold       350,000 GB â†’ $  1,400.00
  Archive    250,000 GB â†’ $    900.00
============================================================
```

---

## ðŸ’° OtimizaÃ§Ã£o de Custos

### Storage Pricing (AWS S3 us-east-1)

| Storage Class | Price/GB/month | Use Case | Min Duration | Retrieval Cost |
|---------------|----------------|----------|--------------|----------------|
| **STANDARD** | $0.023 | Hot, frequently accessed | - | Free |
| **STANDARD_IA** | $0.0125 | Warm, infrequent access | 30 days | $0.01/GB |
| **GLACIER_IR** | $0.004 | Cold, instant retrieval | 90 days | $0.03/GB |
| **GLACIER** | $0.0036 | Archive, retrieval in minutes | 90 days | $0.01/GB + $0.03/1000 req |
| **DEEP_ARCHIVE** | $0.00099 | Long-term archive | 180 days | $0.02/GB + 12h retrieval |

### Exemplo Real: Netflix Scale

**Assumptions:**
- Total content: 5 PB (5,000,000 GB)
- Content distribution:
  - 10% hot (new releases, trending): 500 TB
  - 20% warm (recent, popular): 1 PB
  - 40% cold (catalog): 2 PB
  - 30% archive (old catalog): 1.5 PB

**Cost Calculation:**

```python
# Without lifecycle policies (all Standard)
cost_no_lifecycle = 5_000_000 * 0.023 = $115,000/month

# With lifecycle policies
cost_hot = 500_000 * 0.023 = $11,500
cost_warm = 1_000_000 * 0.0125 = $12,500
cost_cold = 2_000_000 * 0.004 = $8,000
cost_archive = 1_500_000 * 0.0036 = $5,400

cost_with_lifecycle = $37,400/month

# Savings
monthly_savings = $115,000 - $37,400 = $77,600 (67.5%)
annual_savings = $931,200
```

**ROI:**
- **Monthly savings: $77,600**
- **Annual savings: $931,200**
- **Savings: 67.5%**

### CDN Cost Optimization

**Without CDN:**
- All requests go to S3 origin
- Data egress: $0.09/GB (first 10 TB)
- For 10 PB/month egress: ~$900,000/month

**With CloudFront:**
- Cache hit ratio: 95%
- Only 5% requests hit origin
- CloudFront egress: $0.085/GB (cheaper than S3)
- Origin egress: $0.09/GB for 5% = 500 TB

```
Cost without CDN: $900,000/month
Cost with CDN: $850,000 (CloudFront) + $45,000 (5% origin) = $895,000
Net cost: ~$895,000 (similar cost but MUCH better performance)

Additional savings from caching:
- 95% reduction in origin load
- Faster response times globally
- Lower database load
```

### Multipart Upload Optimization

Para arquivos grandes (>100MB), use multipart upload:

**Benefits:**
- 5-10x faster upload
- Resume capability
- Parallel chunk uploads
- Better network utilization

**Implementation:**

```python
# storage_manager.py already implements this automatically
# Files >100MB use multipart with:
config = boto3.s3.transfer.TransferConfig(
    multipart_threshold=100 * 1024**2,  # 100MB
    max_concurrency=10,  # 10 parallel uploads
    multipart_chunksize=10 * 1024**2  # 10MB chunks
)
```

---

## ðŸ”§ Setup e ConfiguraÃ§Ã£o

### AWS S3 Setup

```bash
# 1. Create buckets
aws s3 mb s3://netflix-raw-content
aws s3 mb s3://netflix-encoded-content
aws s3 mb s3://netflix-cdn-content

# 2. Enable versioning (optional)
aws s3api put-bucket-versioning \
  --bucket netflix-encoded-content \
  --versioning-configuration Status=Enabled

# 3. Enable encryption
aws s3api put-bucket-encryption \
  --bucket netflix-encoded-content \
  --server-side-encryption-configuration '{
    "Rules": [{
      "ApplyServerSideEncryptionByDefault": {
        "SSEAlgorithm": "AES256"
      }
    }]
  }'

# 4. Configure lifecycle policies
python -c "
from lifecycle_policies import S3LifecycleManager
lm = S3LifecycleManager('netflix-encoded-content')
lm.create_content_lifecycle_policy()
"

# 5. Configure CloudFront
# (Use AWS Console or CloudFormation)
```

### Google Cloud Storage Setup

```bash
# 1. Create buckets
gsutil mb -c STANDARD -l us-east1 gs://netflix-raw-content
gsutil mb -c STANDARD -l us-east1 gs://netflix-encoded-content

# 2. Enable uniform bucket-level access
gsutil uniformbucketlevelaccess set on gs://netflix-encoded-content

# 3. Configure lifecycle
python -c "
from lifecycle_policies import GCSLifecycleManager
lm = GCSLifecycleManager('netflix-encoded-content')
lm.create_content_lifecycle_policy()
"

# 4. Configure Cloud CDN
# (Use GCP Console or Terraform)
```

### Python Dependencies

```bash
pip install boto3 google-cloud-storage
```

### Environment Variables

```bash
# AWS
export AWS_ACCESS_KEY_ID="your_key"
export AWS_SECRET_ACCESS_KEY="your_secret"
export AWS_DEFAULT_REGION="us-east-1"

# GCP
export GOOGLE_APPLICATION_CREDENTIALS="/path/to/credentials.json"
```

---

## ðŸ“Š Monitoring e MÃ©tricas

### CloudWatch Metrics (S3)

```python
import boto3

cloudwatch = boto3.client('cloudwatch')

# Get bucket size
response = cloudwatch.get_metric_statistics(
    Namespace='AWS/S3',
    MetricName='BucketSizeBytes',
    Dimensions=[
        {'Name': 'BucketName', 'Value': 'netflix-content'},
        {'Name': 'StorageType', 'Value': 'StandardStorage'}
    ],
    StartTime=datetime.utcnow() - timedelta(days=1),
    EndTime=datetime.utcnow(),
    Period=86400,
    Statistics=['Average']
)

size_gb = response['Datapoints'][0]['Average'] / (1024**3)
print(f"Bucket size: {size_gb:.2f} GB")
```

### MÃ©tricas Importantes

- **Bucket Size**: Total storage por storage class
- **Number of Objects**: Total de objetos armazenados
- **Requests**: GET, PUT, DELETE requests/dia
- **Data Egress**: GB transferred/dia
- **4xx/5xx Errors**: Error rates

### Cost Monitoring

```bash
# AWS Cost Explorer (CLI)
aws ce get-cost-and-usage \
  --time-period Start=2024-01-01,End=2024-01-31 \
  --granularity MONTHLY \
  --metrics UnblendedCost \
  --group-by Type=SERVICE

# Storage cost analysis
python lifecycle_policies.py
```

---

## ðŸ”’ SeguranÃ§a

### Encryption at Rest

**S3:**
```python
# Server-side encryption (automatic)
extra_args = {
    'ServerSideEncryption': 'AES256'
}

# Or use KMS
extra_args = {
    'ServerSideEncryption': 'aws:kms',
    'SSEKMSKeyId': 'arn:aws:kms:us-east-1:123:key/...'
}
```

**GCS:**
```python
# Default encryption (automatic)
# Or customer-managed encryption keys (CMEK)
blob.kms_key_name = 'projects/.../locations/.../keyRings/.../cryptoKeys/...'
```

### Access Control

**S3 Bucket Policy:**

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Deny",
      "Principal": "*",
      "Action": "s3:PutObject",
      "Resource": "arn:aws:s3:::netflix-content/*",
      "Condition": {
        "StringNotEquals": {
          "s3:x-amz-server-side-encryption": "AES256"
        }
      }
    }
  ]
}
```

### Presigned URLs

```python
# Generate temporary access URL (1 hour)
url = storage.get_url(
    key='content/movie_123/master.m3u8',
    temporary=True,
    expires_in=3600
)

# Use for:
# - Direct browser uploads
# - Time-limited downloads
# - Third-party integrations
```

---

## ðŸ§ª Testing

```bash
# Unit tests
pytest tests/test_storage_manager.py

# Integration test with real S3
python -m pytest tests/integration/ --s3

# Load test (1000 uploads)
python tests/load_test_storage.py --files 1000 --parallel 50
```

---

## ðŸ“š Recursos

### DocumentaÃ§Ã£o

- [AWS S3 Documentation](https://docs.aws.amazon.com/s3/)
- [S3 Lifecycle Configuration](https://docs.aws.amazon.com/AmazonS3/latest/userguide/object-lifecycle-mgmt.html)
- [Google Cloud Storage Documentation](https://cloud.google.com/storage/docs)
- [CloudFront Documentation](https://docs.aws.amazon.com/cloudfront/)

### Tools

- [AWS CLI](https://aws.amazon.com/cli/)
- [gsutil](https://cloud.google.com/storage/docs/gsutil)
- [s3cmd](https://s3tools.org/s3cmd)

### Best Practices

- [S3 Performance Best Practices](https://docs.aws.amazon.com/AmazonS3/latest/userguide/optimizing-performance.html)
- [S3 Cost Optimization](https://aws.amazon.com/s3/cost-optimization/)
- [Netflix Cloud Storage Architecture](https://netflixtechblog.com/)

---

**Storage layer com multi-cloud e otimizaÃ§Ã£o de custos pronto! ðŸ’¾**

**Savings:** 65-70% em storage costs com lifecycle policies automÃ¡ticas.
