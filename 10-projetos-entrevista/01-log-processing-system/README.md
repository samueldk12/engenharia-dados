# Log Processing System

**Pergunta de Entrevista:** "Como voc√™ processaria 10GB de logs por segundo de forma eficiente?"

## üìã Requisitos

### Funcionais
1. **Parse de Logs**: Suportar m√∫ltiplos formatos (Apache, Nginx, JSON, custom)
2. **Agrega√ß√£o**: M√©tricas em janelas de tempo (1min, 5min, 1hora)
3. **Detec√ß√£o de Anomalias**: Identificar padr√µes anormais
4. **Persist√™ncia**: Escrita eficiente para storage (batch writes)

### N√£o-Funcionais
1. **Throughput**: Processar 10GB/s (‚âà 100K logs/sec @ 100KB/log)
2. **Lat√™ncia**: <100ms para agrega√ß√µes em tempo real
3. **Memory**: <2GB para 1 hora de dados na mem√≥ria
4. **CPU**: <50% de uma CPU por 10K logs/sec

## üéØ Back-of-the-Envelope Calculations

```
# Assumptions
Tamanho m√©dio de log: 100 bytes
Logs por segundo: 100,000
Tamanho total: 100,000 * 100 bytes = 10 MB/s = 36 GB/hour

# Memory para agrega√ß√µes
Unique IPs: ~10K
Unique URLs: ~100K
Unique User-Agents: ~1K
Agrega√ß√µes por m√©trica: 8 bytes (long) + 50 bytes (key)
Total por minuto: 111K * 58 bytes ‚âà 6.4 MB
Total por hora (60 min): 6.4 MB * 60 = 384 MB ‚úÖ

# CPU
Parse com regex: ~1000 logs/sec por core
Parse com split: ~10,000 logs/sec por core
Para 100K logs/sec: precisa 10-100 cores dependendo da estrat√©gia
```

## üèóÔ∏è Arquitetura

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Log Source ‚îÇ  (Apache, Nginx, App logs)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Parser    ‚îÇ  Regex vs Split (10x difference)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Aggregator ‚îÇ  Sliding windows (1min, 5min, 1h)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Detector  ‚îÇ  Anomaly detection (threshold, z-score)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Writer    ‚îÇ  Batch writes (1000 logs/batch)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üîß Implementa√ß√£o

### 1. Log Parser (log_parser.py)

**Desafio**: Parse eficiente √© cr√≠tico para performance

**Estrat√©gias**:
- ‚ùå **Regex**: Lento (1K logs/sec)
- ‚úÖ **Split + Manual**: R√°pido (10K logs/sec)
- ‚úÖ‚úÖ **Compilar Regex**: Meio-termo (5K logs/sec)

```python
# Apache Log Format
# 127.0.0.1 - - [10/Oct/2000:13:55:36 -0700] "GET /index.html HTTP/1.0" 200 2326

# Naive Regex (LENTO - 1K logs/sec)
pattern = r'(\d+\.\d+\.\d+\.\d+) .* \[(.*?)\] "(.*?)" (\d+) (\d+)'

# Otimizado: Split (R√ÅPIDO - 10K logs/sec)
parts = line.split(' ')
ip = parts[0]
timestamp = parts[3][1:]  # Remove '['
method_path = parts[5:8]
status = int(parts[8])
size = int(parts[9])
```

### 2. Aggregator (aggregator.py)

**Desafio**: Manter agrega√ß√µes em m√∫ltiplas janelas de tempo

**Data Structure**:
```python
# Sliding Window com Deque
from collections import deque, defaultdict

class SlidingWindowAggregator:
    def __init__(self, window_size_sec=60):
        self.windows = defaultdict(deque)  # key -> [(timestamp, value), ...]
        self.aggregations = defaultdict(int)  # key -> sum

    def add(self, key, value, timestamp):
        # O(1) - Adiciona ao final
        self.windows[key].append((timestamp, value))
        self.aggregations[key] += value

        # Remove valores antigos - O(k) onde k √© # de valores expirados
        cutoff = timestamp - self.window_size_sec
        while self.windows[key] and self.windows[key][0][0] < cutoff:
            old_ts, old_val = self.windows[key].popleft()
            self.aggregations[key] -= old_val
```

**Complexidade**:
- Add: O(1) amortizado
- Get: O(1)
- Memory: O(n) onde n = eventos na janela

### 3. Anomaly Detector (anomaly_detector.py)

**Estrat√©gias**:

**a) Threshold-based (Simples)**
```python
if current_rate > threshold:
    alert("High rate detected")
```

**b) Z-Score (Estat√≠stico)**
```python
# Detecta quando valor est√° X desvios-padr√£o da m√©dia
z_score = (current - mean) / std_dev
if abs(z_score) > 3:  # 3-sigma rule
    alert("Anomaly detected")
```

**c) Moving Average (Suavizado)**
```python
# Detecta mudan√ßas bruscas
if abs(current - moving_avg) > threshold * moving_avg:
    alert("Spike detected")
```

### 4. Writer (writer.py)

**Desafio**: Escrita eficiente para minimizar I/O

**Estrat√©gias**:
```python
# ‚ùå Write-per-log (LENTO - 100 logs/sec)
for log in logs:
    file.write(log)  # 1 I/O operation per log

# ‚úÖ Batch Write (R√ÅPIDO - 10K logs/sec)
buffer = []
for log in logs:
    buffer.append(log)
    if len(buffer) >= BATCH_SIZE:  # e.g., 1000
        file.write('\n'.join(buffer))  # 1 I/O per 1000 logs
        buffer.clear()

# ‚úÖ‚úÖ Async Write (MUITO R√ÅPIDO - 50K logs/sec)
import asyncio
import aiofiles

async def write_batch(logs):
    async with aiofiles.open('output.log', mode='a') as f:
        await f.write('\n'.join(logs))
```

## üìä Benchmarks

| Estrat√©gia | Throughput | Lat√™ncia | Memory |
|------------|------------|----------|--------|
| Regex Parse | 1K logs/sec | 1ms | 50 MB |
| Split Parse | 10K logs/sec | 0.1ms | 50 MB |
| Batch Write (100) | 5K logs/sec | 20ms | 10 MB |
| Batch Write (1000) | 10K logs/sec | 100ms | 100 MB |
| Async Write | 50K logs/sec | 20ms | 100 MB |

## üéØ Otimiza√ß√µes

### 1. Multi-threading
```python
from concurrent.futures import ThreadPoolExecutor

# Processar logs em paralelo
with ThreadPoolExecutor(max_workers=10) as executor:
    futures = [executor.submit(process_log, log) for log in logs]
    results = [f.result() for f in futures]
```

**Trade-off**: GIL em Python limita paralelismo. Melhor usar multiprocessing.

### 2. Multi-processing
```python
from multiprocessing import Pool

# Processar logs em m√∫ltiplos processos
with Pool(processes=10) as pool:
    results = pool.map(process_log_batch, log_batches)
```

**Ganho**: 10x throughput (se 10 cores dispon√≠veis)

### 3. Memory-Mapped Files
```python
import mmap

# Ler arquivo sem carregar tudo na mem√≥ria
with open('huge.log', 'r+b') as f:
    with mmap.mmap(f.fileno(), 0) as mmap_obj:
        for line in iter(mmap_obj.readline, b""):
            process_line(line)
```

**Ganho**: Processa arquivos de 100GB com <1GB RAM

### 4. Columnar Storage (Parquet)
```python
import pyarrow.parquet as pq

# Escrever em formato columnar para queries r√°pidas
df.to_parquet('logs.parquet', compression='snappy')

# Ler apenas colunas necess√°rias
df = pq.read_table('logs.parquet', columns=['ip', 'status']).to_pandas()
```

**Ganho**: 10x compress√£o, 10x velocidade de leitura para queries anal√≠ticas

## üöÄ Scaling para 100GB/s

### Horizontal Scaling

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Kafka     ‚îÇ  (Sharded por IP hash)
‚îÇ  (100 parts)‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
       ‚ñº                           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Consumer 1 ‚îÇ            ‚îÇ  Consumer N ‚îÇ  (100 consumers)
‚îÇ  (Spark)    ‚îÇ            ‚îÇ  (Spark)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ                           ‚îÇ
       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                   ‚ñº
           ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
           ‚îÇ  S3/HDFS    ‚îÇ  (Parquet format)
           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Capacidade**:
- Kafka: 100 partitions √ó 1GB/s = 100 GB/s
- Spark: 100 executors √ó 1GB/s = 100 GB/s
- S3: Unlimited throughput com prefixes diferentes

### Estimativa de Recursos

```python
# Para 100GB/s (1M logs/sec @ 100KB/log)

# Kafka
Partitions: 100
Brokers: 10 (cada broker 10GB/s)
Reten√ß√£o: 7 dias √ó 100GB/s √ó 86400 sec = 60 PB

# Spark
Executors: 100
Cores por executor: 4
Memory por executor: 16 GB
Total: 400 cores, 1.6 TB memory

# Storage (S3)
Daily: 100GB/s √ó 86400 sec = 8.6 PB/dia
Compress√£o Snappy: 3x ‚Üí 2.9 PB/dia
Mensal: 87 PB
Anual: 1 EB (ExaByte!)

# Custo
S3: $0.023/GB √ó 87 PB = $2M/m√™s
Spark (100 r5.4xlarge): $1.00/hr √ó 100 √ó 730 hrs = $73K/m√™s
Total: ~$2.1M/m√™s
```

## üß™ Testes

```bash
# Executar testes
pytest tests/ -v

# Benchmarks
python benchmarks/benchmark_parser.py
python benchmarks/benchmark_aggregator.py

# Teste de carga
python benchmarks/load_test.py --rate 10000  # 10K logs/sec
```

## üìù Perguntas de Follow-up

### Q1: Como garantir exactly-once processing?

**Resposta**:
```python
# Usar idempotent writes + offset tracking

# 1. Cada log tem ID √∫nico
log_id = f"{timestamp}_{ip}_{sequence}"

# 2. Track offset no Kafka
consumer.commit()  # Commit apenas ap√≥s sucesso

# 3. Deduplica√ß√£o no sink
# Usar UPSERT baseado em log_id
INSERT INTO logs (...) ON CONFLICT (log_id) DO NOTHING
```

### Q2: Como lidar com logs out-of-order?

**Resposta**:
```python
# Usar watermark + grace period

# 1. Watermark: √∫ltimo timestamp processado
watermark = max_timestamp - grace_period  # e.g., -5min

# 2. Buffer de logs tardios
late_buffer = []  # Logs com timestamp < watermark

# 3. Processar tardios em batch separado
if timestamp < watermark:
    late_buffer.append(log)
else:
    process(log)
```

### Q3: Como otimizar para queries anal√≠ticas?

**Resposta**:
```python
# 1. Particionamento por data
s3://bucket/year=2024/month=01/day=15/part-00001.parquet

# 2. Compress√£o columnar
# Parquet com Snappy: 3x compress√£o

# 3. Pr√©-agrega√ß√µes
# Materializar agrega√ß√µes comuns (por hora, por URL, por status)

# 4. Indexa√ß√£o
# Bloom filters para IP lookups
# Z-ordering para multi-dimensional queries
```

## üéì Conceitos-Chave

1. **String Parsing**: Regex vs Split (10x difference)
2. **Sliding Windows**: Deque para O(1) add/remove
3. **Batch Processing**: Reduzir I/O operations
4. **Async I/O**: aiofiles para escrita n√£o-bloqueante
5. **Columnar Storage**: Parquet para analytics
6. **Partitioning**: Por data para queries eficientes
7. **Compression**: Snappy (r√°pido) vs Gzip (compacto)
8. **Multiprocessing**: Contornar GIL do Python

## ‚ö†Ô∏è Red Flags na Entrevista

‚ùå **Usar regex sem compilar**
‚ùå **N√£o mencionar batch processing**
‚ùå **Ignorar memory footprint**
‚ùå **N√£o considerar paraleliza√ß√£o**
‚ùå **Esquecer de tratar logs mal-formados**
‚ùå **N√£o falar sobre monitoramento**

‚úÖ **Bom candidato fala sobre**:
- Parse otimizado (split vs regex)
- Batch writes
- Memory-efficient aggregations
- Horizontal scaling (Kafka + Spark)
- Monitoring e alerting

## üèÜ Solu√ß√£o Completa

Ver arquivos:
- `log_parser.py` - Parser otimizado
- `aggregator.py` - Sliding window aggregations
- `anomaly_detector.py` - Detec√ß√£o de anomalias
- `writer.py` - Batch writer
- `main.py` - Orquestra√ß√£o completa
- `benchmarks/` - Performance tests

**Tempo de implementa√ß√£o**: 45-60 minutos
**Dificuldade**: ‚≠ê‚≠ê (Medium)
