# Rate Limiter

**Pergunta de Entrevista:** "Implemente um rate limiter distribu√≠do que suporte 1000 requests/min por usu√°rio com <1ms de overhead"

## üìã Requisitos

### Funcionais
1. **Limit Requests**: Limitar N requests por janela de tempo
2. **Per-User**: Limites por usu√°rio/IP/API key
3. **Multiple Windows**: Suportar m√∫ltiplas janelas (1min, 1hour, 1day)
4. **Reject or Queue**: Rejeitar ou colocar em fila requests excedentes
5. **Distributed**: Funcionar em m√∫ltiplos servidores

### N√£o-Funcionais
1. **Lat√™ncia**: <1ms overhead (p99)
2. **Throughput**: 100K requests/sec
3. **Precis√£o**: ¬±1% de erro aceit√°vel
4. **Memory**: O(U) onde U = n√∫mero de usu√°rios ativos
5. **Availability**: 99.99% uptime

## üéØ Back-of-the-Envelope Calculations

```
# Assumptions
Users ativos: 1M
Requests por user: 1000/min = ~17 req/sec
Total: 1M √ó 17 = 17M req/sec

# Memory (Redis)
Por user: user_id (8 bytes) + counter (8 bytes) + timestamp (8 bytes) = 24 bytes
Total: 1M √ó 24 bytes = 24 MB ‚úÖ

# Lat√™ncia
Redis GET/INCR: ~0.1ms
Network RTT: ~0.5ms
Processing: ~0.1ms
Total: ~0.7ms ‚úÖ (<1ms)

# Redis throughput
Single instance: ~100K ops/sec
Para 17M req/sec: precisa ~170 Redis instances
Com clustering: 10 instances √ó 100K = 1M ops/sec ‚úÖ
```

## üèóÔ∏è Algoritmos de Rate Limiting

### 1. Token Bucket ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê

**Mais usado na ind√∫stria (AWS, Stripe, Cloudflare)**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Token Bucket   ‚îÇ  Capacity: 100 tokens
‚îÇ                  ‚îÇ  Refill: 10 tokens/sec
‚îÇ  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñë‚ñë‚ñë‚ñë‚ñë   ‚îÇ  Current: 60 tokens
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Request ‚Üí Consume 1 token
If tokens ‚â• 1: ALLOW, tokens--
Else: REJECT

Refill: tokens = min(capacity, tokens + rate √ó elapsed)
```

**Vantagens**:
- ‚úÖ Permite bursts (at√© capacity)
- ‚úÖ Smooth rate limiting
- ‚úÖ Memory efficient: O(1) per user

**Desvantagens**:
- ‚ùå Requer lock para updates (distributed)
- ‚ùå Clock synchronization issues

**Implementa√ß√£o**:
```python
class TokenBucket:
    def __init__(self, capacity: int, refill_rate: float):
        self.capacity = capacity
        self.tokens = capacity
        self.refill_rate = refill_rate  # tokens per second
        self.last_refill = time.time()

    def allow_request(self) -> bool:
        # Refill tokens
        now = time.time()
        elapsed = now - self.last_refill
        self.tokens = min(
            self.capacity,
            self.tokens + self.refill_rate * elapsed
        )
        self.last_refill = now

        # Consume token
        if self.tokens >= 1:
            self.tokens -= 1
            return True
        return False
```

**Complexidade**:
- Time: O(1)
- Space: O(1)

---

### 2. Sliding Window Log ‚≠ê‚≠ê‚≠ê‚≠ê

**Usado quando precis√£o √© cr√≠tica**

```
Window: Last 60 seconds
Log: [t1, t2, t3, ..., tn]  (timestamps of requests)

Request at time T:
1. Remove timestamps < (T - 60)
2. If len(log) < limit: ALLOW, append T
3. Else: REJECT
```

**Vantagens**:
- ‚úÖ Precis√£o perfeita
- ‚úÖ Sem edge effects

**Desvantagens**:
- ‚ùå Memory intensive: O(N) per user onde N = requests na janela
- ‚ùå Slow: O(N) para cleanup

**Implementa√ß√£o**:
```python
from collections import deque

class SlidingWindowLog:
    def __init__(self, limit: int, window_sec: int):
        self.limit = limit
        self.window_sec = window_sec
        self.log = deque()  # timestamps

    def allow_request(self) -> bool:
        now = time.time()

        # Remove old timestamps - O(k)
        cutoff = now - self.window_sec
        while self.log and self.log[0] < cutoff:
            self.log.popleft()

        # Check limit
        if len(self.log) < self.limit:
            self.log.append(now)
            return True
        return False
```

**Complexidade**:
- Time: O(k) onde k = expired requests
- Space: O(N) onde N = requests na janela

---

### 3. Fixed Window Counter ‚≠ê‚≠ê‚≠ê

**Mais simples, usado para casos n√£o-cr√≠ticos**

```
Window: 1 minute chunks

Minute 1: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100 requests ‚Üí ALLOW
Minute 2: [‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà] 100 requests ‚Üí ALLOW
Edge: 99 req at 00:59 + 100 req at 01:01 = 199 req in 2 seconds! ‚ùå
```

**Vantagens**:
- ‚úÖ Muito simples
- ‚úÖ Memory efficient: O(1)
- ‚úÖ Fast: O(1)

**Desvantagens**:
- ‚ùå Edge effect: pode permitir 2√ó limit em window boundaries
- ‚ùå N√£o permite bursts

**Implementa√ß√£o**:
```python
class FixedWindowCounter:
    def __init__(self, limit: int, window_sec: int):
        self.limit = limit
        self.window_sec = window_sec
        self.counter = 0
        self.window_start = time.time()

    def allow_request(self) -> bool:
        now = time.time()

        # Reset counter if new window
        if now - self.window_start >= self.window_sec:
            self.counter = 0
            self.window_start = now

        # Check limit
        if self.counter < self.limit:
            self.counter += 1
            return True
        return False
```

---

### 4. Sliding Window Counter ‚≠ê‚≠ê‚≠ê‚≠ê

**Hybrid: precis√£o melhor que Fixed, mais eficiente que Log**

```
Current Window: 60% into minute 2

Minute 1: 80 requests
Minute 2: 60 requests (at√© agora)

Estimated count = 80 √ó (1 - 0.6) + 60 = 32 + 60 = 92
If 92 < 100: ALLOW
```

**Vantagens**:
- ‚úÖ Boa precis√£o (¬±5%)
- ‚úÖ Memory efficient: O(1)
- ‚úÖ Fast: O(1)

**Desvantagens**:
- ‚ùå Aproxima√ß√£o (n√£o exata)

**Implementa√ß√£o**:
```python
class SlidingWindowCounter:
    def __init__(self, limit: int, window_sec: int):
        self.limit = limit
        self.window_sec = window_sec
        self.current_window = {'start': time.time(), 'count': 0}
        self.previous_count = 0

    def allow_request(self) -> bool:
        now = time.time()

        # Check if need new window
        if now - self.current_window['start'] >= self.window_sec:
            self.previous_count = self.current_window['count']
            self.current_window = {'start': now, 'count': 0}

        # Calculate weighted count
        elapsed = now - self.current_window['start']
        weight = 1 - (elapsed / self.window_sec)
        estimated_count = (
            self.previous_count * weight +
            self.current_window['count']
        )

        # Check limit
        if estimated_count < self.limit:
            self.current_window['count'] += 1
            return True
        return False
```

---

## üî¥ Redis-Based Distributed Rate Limiter

**Problema**: Rate limiter local n√£o funciona com m√∫ltiplos servidores

**Solu√ß√£o**: Usar Redis como shared counter

### Token Bucket com Redis

```python
import redis
import time

class DistributedTokenBucket:
    def __init__(
        self,
        redis_client: redis.Redis,
        capacity: int,
        refill_rate: float
    ):
        self.redis = redis_client
        self.capacity = capacity
        self.refill_rate = refill_rate

    def allow_request(self, user_id: str) -> bool:
        key = f"rate_limit:{user_id}"

        # Lua script para atomicidade (ACID)
        lua_script = """
        local key = KEYS[1]
        local capacity = tonumber(ARGV[1])
        local refill_rate = tonumber(ARGV[2])
        local now = tonumber(ARGV[3])

        -- Get current state
        local state = redis.call('HMGET', key, 'tokens', 'last_refill')
        local tokens = tonumber(state[1]) or capacity
        local last_refill = tonumber(state[2]) or now

        -- Refill tokens
        local elapsed = now - last_refill
        tokens = math.min(capacity, tokens + refill_rate * elapsed)

        -- Try to consume token
        if tokens >= 1 then
            tokens = tokens - 1
            redis.call('HMSET', key, 'tokens', tokens, 'last_refill', now)
            redis.call('EXPIRE', key, 3600)  -- TTL 1 hour
            return 1  -- ALLOW
        else
            return 0  -- REJECT
        end
        """

        result = self.redis.eval(
            lua_script,
            1,  # num keys
            key,
            self.capacity,
            self.refill_rate,
            time.time()
        )

        return bool(result)
```

**Por que Lua Script?**
- ‚úÖ **Atomic**: Toda opera√ß√£o √© at√¥mica (read + update)
- ‚úÖ **Fast**: Executa no servidor Redis (sem round-trips)
- ‚úÖ **Consistent**: N√£o h√° race conditions

---

### Fixed Window com Redis (Mais Simples)

```python
class DistributedFixedWindow:
    def __init__(self, redis_client: redis.Redis, limit: int, window_sec: int):
        self.redis = redis_client
        self.limit = limit
        self.window_sec = window_sec

    def allow_request(self, user_id: str) -> bool:
        # Key format: rate_limit:user:123:window:1609459200
        window_start = int(time.time() // self.window_sec) * self.window_sec
        key = f"rate_limit:{user_id}:{window_start}"

        # Atomic increment
        current = self.redis.incr(key)

        # Set TTL on first request
        if current == 1:
            self.redis.expire(key, self.window_sec * 2)

        return current <= self.limit
```

**Vantagens**:
- ‚úÖ Muito simples (2 Redis commands)
- ‚úÖ Fast: ~0.5ms latency
- ‚úÖ Memory efficient

---

## üìä Compara√ß√£o de Algoritmos

| Algoritmo | Precis√£o | Memory | Latency | Bursts | Distribu√≠do |
|-----------|----------|--------|---------|--------|-------------|
| **Token Bucket** | ‚≠ê‚≠ê‚≠ê‚≠ê | O(1) | O(1) | ‚úÖ | ‚úÖ (Redis) |
| **Sliding Window Log** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | O(N) | O(N) | ‚úÖ | ‚ö†Ô∏è (Dif√≠cil) |
| **Fixed Window** | ‚≠ê‚≠ê | O(1) | O(1) | ‚ùå | ‚úÖ (Redis) |
| **Sliding Counter** | ‚≠ê‚≠ê‚≠ê‚≠ê | O(1) | O(1) | ‚≠ê | ‚úÖ (Redis) |

**Recomenda√ß√£o**:
- **APIs p√∫blicas**: Token Bucket (permite bursts leg√≠timos)
- **Prote√ß√£o contra abuse**: Fixed Window (mais simples)
- **Billing/Quotas**: Sliding Window Log (precis√£o perfeita)

---

## üöÄ Otimiza√ß√µes

### 1. Local Cache + Redis (Hybrid)

```python
class HybridRateLimiter:
    """
    - Local cache para requests recentes (<1 second)
    - Redis para state compartilhado

    Reduz Redis calls em ~90%
    """
    def __init__(self, redis_client, capacity, refill_rate):
        self.redis_limiter = DistributedTokenBucket(...)
        self.local_cache = {}  # user_id -> (tokens, timestamp)
        self.cache_ttl = 1  # 1 second

    def allow_request(self, user_id: str) -> bool:
        # Try local cache first
        if user_id in self.local_cache:
            tokens, ts = self.local_cache[user_id]
            if time.time() - ts < self.cache_ttl:
                if tokens >= 1:
                    self.local_cache[user_id] = (tokens - 1, ts)
                    return True
                return False

        # Fallback to Redis
        allowed = self.redis_limiter.allow_request(user_id)

        # Update local cache
        if allowed:
            self.local_cache[user_id] = (self.capacity - 1, time.time())

        return allowed
```

**Ganho**: 90% redu√ß√£o em Redis calls, <0.1ms latency (cached)

---

### 2. Bloom Filter para Blocked Users

```python
from pybloom_live import BloomFilter

class OptimizedRateLimiter:
    """
    Se user est√° definitivamente bloqueado (hit limit),
    n√£o precisa consultar Redis
    """
    def __init__(self, capacity=10000, error_rate=0.001):
        self.bloom = BloomFilter(capacity, error_rate)
        self.limiter = DistributedTokenBucket(...)

    def allow_request(self, user_id: str) -> bool:
        # Fast path: definitivamente bloqueado
        if user_id in self.bloom:
            return False

        # Slow path: consultar Redis
        allowed = self.limiter.allow_request(user_id)

        if not allowed:
            self.bloom.add(user_id)

        return allowed
```

**Ganho**: Evita Redis lookup para usu√°rios bloqueados

---

## üß™ Benchmarks

```python
# Single-threaded
Token Bucket (local):     1,000,000 req/sec
Fixed Window (local):     1,200,000 req/sec
Sliding Log (local):        100,000 req/sec

# Redis-based
Token Bucket (Redis):       50,000 req/sec  (limited by Redis)
Fixed Window (Redis):       80,000 req/sec
With local cache:          500,000 req/sec  (10x improvement)

# Latency (p99)
Local:                      0.01 ms
Redis:                      0.8 ms
Redis + local cache:        0.05 ms
```

---

## üìù Perguntas de Follow-up

### Q1: Como escalar para 100M requests/sec?

**Resposta**:
```
1. Redis Cluster (sharding por user_id hash)
   - 100 shards √ó 1M req/sec = 100M req/sec

2. Local cache agressivo (5-10 seconds)
   - Reduz Redis load em 95%

3. Rate limit por tier
   - Free users: strict limit
   - Paid users: lenient limit
   - Distribuir load

4. CDN edge rate limiting
   - Cloudflare/Fastly rate limit
   - Antes de chegar ao seu servidor
```

### Q2: Como garantir fairness entre usu√°rios?

**Resposta**:
```python
# Problema: Burst traffic de poucos usu√°rios pode impactar outros

# Solu√ß√£o: Global rate limit + per-user limit

class FairRateLimiter:
    def __init__(self):
        self.per_user_limit = 1000  # per minute
        self.global_limit = 100000  # per minute
        self.user_limiters = {}
        self.global_counter = 0

    def allow_request(self, user_id):
        # Check global limit first
        if self.global_counter >= self.global_limit:
            return False

        # Check per-user limit
        if user_id not in self.user_limiters:
            self.user_limiters[user_id] = TokenBucket(...)

        if self.user_limiters[user_id].allow_request():
            self.global_counter += 1
            return True

        return False
```

### Q3: Como lidar com clock skew em distributed systems?

**Resposta**:
```
1. NTP Sync: Sincronizar clocks (<1ms drift)

2. Logical Clocks: Usar sequence numbers em vez de timestamps
   counter = Redis INCR
   N√£o depende de timestamps precisos

3. Relaxar requisitos: ¬±5% error √© aceit√°vel para rate limiting

4. Usar Redis como single source of truth
   Redis timestamps s√£o consistentes
```

---

## üéì Conceitos-Chave

1. **Atomicidade**: Lua scripts no Redis
2. **CAP Theorem**: Escolher entre consistency e availability
3. **Caching**: Local cache para reduzir lat√™ncia
4. **Sharding**: Distribuir load entre m√∫ltiplos Redis
5. **Trade-offs**: Precis√£o vs Performance

---

## ‚ö†Ô∏è Red Flags na Entrevista

‚ùå **N√£o mencionar atomicidade** (race conditions)
‚ùå **Esquecer de TTL no Redis** (memory leak)
‚ùå **N√£o discutir distributed challenges**
‚ùå **Ignorar edge effects** (Fixed Window)
‚ùå **N√£o otimizar lat√™ncia** (local cache)

‚úÖ **Bom candidato fala sobre**:
- M√∫ltiplos algoritmos e trade-offs
- Atomicidade com Lua scripts
- Local cache para otimiza√ß√£o
- Sharding para scale
- Clock synchronization issues

---

## üèÜ Solu√ß√£o Completa

Ver arquivos:
- `strategies/token_bucket.py` - Token Bucket
- `strategies/sliding_window.py` - Sliding Window Log
- `strategies/fixed_window.py` - Fixed Window Counter
- `distributed_limiter.py` - Redis-based distribu√≠do
- `decorator.py` - Python decorator para APIs
- `benchmarks/compare.py` - Benchmark de todas estrat√©gias

**Tempo de implementa√ß√£o**: 60 minutos
**Dificuldade**: ‚≠ê‚≠ê‚≠ê (Medium-Hard)
**Empresas**: Stripe, Shopify, Cloudflare, Twitter, Reddit
