# ‚ö° Apache Spark: Fundamentos e Arquitetura

## üìã √çndice

1. [O que √© Apache Spark](#o-que-√©-apache-spark)
2. [Arquitetura do Spark](#arquitetura-do-spark)
3. [RDDs vs DataFrames vs Datasets](#rdds-vs-dataframes-vs-datasets)
4. [Transforma√ß√µes e A√ß√µes](#transforma√ß√µes-e-a√ß√µes)
5. [Spark SQL](#spark-sql)
6. [Otimiza√ß√µes e Best Practices](#otimiza√ß√µes-e-best-practices)
7. [Spark Streaming](#spark-streaming)

---

## O que √© Apache Spark

**Apache Spark** √© um engine de processamento distribu√≠do para big data, criado para ser:
- **R√°pido**: At√© 100x mais r√°pido que Hadoop MapReduce (in-memory)
- **Unificado**: Batch, streaming, SQL, ML e graph processing
- **F√°cil de usar**: APIs em Scala, Python, Java, R, SQL

### Por que Spark?

**Antes (Hadoop MapReduce):**
```
Input ‚Üí Map ‚Üí Shuffle ‚Üí Reduce ‚Üí Output (salvo em disco)
Input ‚Üí Map ‚Üí Shuffle ‚Üí Reduce ‚Üí Output (salvo em disco)  # Cada job salva no HDFS
```

**Com Spark:**
```
Input ‚Üí Transform ‚Üí Transform ‚Üí Transform ‚Üí Action (tudo in-memory)
```

**Vantagens:**
- ‚úÖ Processamento in-memory (cache de dados)
- ‚úÖ DAG execution engine (otimiza query plan)
- ‚úÖ APIs de alto n√≠vel (DataFrame, SQL)
- ‚úÖ Lazy evaluation (s√≥ executa quando necess√°rio)

---

## Arquitetura do Spark

### Componentes

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Driver Program                      ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ        SparkContext                   ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ  (coordena execu√ß√£o distribu√≠da)     ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                   ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇ Worker 1 ‚îÇ      ‚îÇ Worker 2 ‚îÇ
   ‚îÇ          ‚îÇ      ‚îÇ          ‚îÇ
   ‚îÇ Executor ‚îÇ      ‚îÇ Executor ‚îÇ
   ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ      ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
   ‚îÇ ‚îÇTask 1‚îÇ ‚îÇ      ‚îÇ ‚îÇTask 3‚îÇ ‚îÇ
   ‚îÇ ‚îÇTask 2‚îÇ ‚îÇ      ‚îÇ ‚îÇTask 4‚îÇ ‚îÇ
   ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ      ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Driver:**
- Executa a aplica√ß√£o Spark (fun√ß√£o main)
- Cria SparkContext
- Converte c√≥digo em DAG de tasks
- Agenda tasks nos executors

**Executors:**
- Processos que executam tasks
- Armazenam dados em cache
- Reportam status ao Driver

**Cluster Manager:**
- YARN, Kubernetes, Mesos, Standalone
- Aloca recursos (CPU, mem√≥ria)

### SparkSession (Spark 2.0+)

```python
from pyspark.sql import SparkSession

# Criar SparkSession (unified entry point)
spark = SparkSession.builder \
    .appName("MyApp") \
    .master("local[4]")  # Local com 4 threads, ou "yarn" para cluster
    .config("spark.executor.memory", "4g") \
    .config("spark.executor.cores", "2") \
    .enableHiveSupport() \
    .getOrCreate()

# SparkSession d√° acesso a:
# - spark.sql()      ‚Üí SQL queries
# - spark.read       ‚Üí Leitura de dados
# - spark.sparkContext ‚Üí RDD operations
```

---

## RDDs vs DataFrames vs Datasets

### 1. RDD (Resilient Distributed Dataset)

**Caracter√≠sticas:**
- API de baixo n√≠vel
- Type-safe (mas sem schema)
- Lazy evaluation
- Imut√°vel

```python
# Criar RDD
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])

# Transforma√ß√µes
rdd2 = rdd.map(lambda x: x * 2)
rdd3 = rdd2.filter(lambda x: x > 5)

# A√ß√£o (trigger execution)
result = rdd3.collect()  # [6, 8, 10]

# RDD de texto
text_rdd = spark.sparkContext.textFile("hdfs://path/to/file.txt")
words = text_rdd.flatMap(lambda line: line.split(" "))
word_counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)
```

**Quando usar RDD:**
- Controle fino sobre particionamento
- Manipula√ß√£o de dados n√£o estruturados
- Algoritmos customizados de baixo n√≠vel

### 2. DataFrame

**Caracter√≠sticas:**
- API de alto n√≠vel (SQL-like)
- Schema definido (colunas tipadas)
- Catalyst optimizer (otimiza query plan)
- Tungsten execution (c√≥digo gerado em tempo de execu√ß√£o)

```python
from pyspark.sql.functions import col, sum, avg, count, when

# Criar DataFrame
data = [
    ("Alice", 25, "Engineer", 100000),
    ("Bob", 30, "Manager", 120000),
    ("Charlie", 35, "Engineer", 110000),
    ("Diana", 28, "Analyst", 90000)
]
df = spark.createDataFrame(data, ["name", "age", "role", "salary"])

# Mostrar schema
df.printSchema()
# root
#  |-- name: string (nullable = true)
#  |-- age: long (nullable = true)
#  |-- role: string (nullable = true)
#  |-- salary: long (nullable = true)

# Opera√ß√µes
df.show()
df.select("name", "salary").show()
df.filter(col("age") > 28).show()
df.groupBy("role").agg(avg("salary").alias("avg_salary")).show()

# SQL-like
df.createOrReplaceTempView("employees")
spark.sql("SELECT role, AVG(salary) FROM employees GROUP BY role").show()
```

### 3. Dataset (Scala/Java only)

**Caracter√≠sticas:**
- Type-safe como RDD
- Otimizado como DataFrame
- N√£o dispon√≠vel em Python (Python tem apenas DataFrame)

```scala
// Scala - Dataset
case class Employee(name: String, age: Int, role: String, salary: Double)

val ds = spark.read.json("employees.json").as[Employee]

// Type-safe operations
ds.filter(emp => emp.age > 30)
  .map(emp => emp.name)
  .show()
```

### Compara√ß√£o

| Feature | RDD | DataFrame | Dataset |
|---------|-----|-----------|---------|
| **Type Safety** | ‚ùå Runtime | ‚ùå Runtime | ‚úÖ Compile-time |
| **Optimization** | ‚ùå No | ‚úÖ Catalyst | ‚úÖ Catalyst |
| **Performance** | Baixa | Alta | Alta |
| **API Level** | Baixo | Alto | Alto |
| **Linguagens** | Todas | Todas | Scala/Java |
| **Usar quando** | Controle fino | Analytics | Type-safety |

**Recomenda√ß√£o:** Use **DataFrame** em 99% dos casos.

---

## Transforma√ß√µes e A√ß√µes

### Lazy Evaluation

Spark usa **lazy evaluation**: transforma√ß√µes s√≥ s√£o executadas quando uma **a√ß√£o** √© chamada.

```python
# Nenhuma execu√ß√£o ainda
df2 = df.filter(col("age") > 30)         # Transforma√ß√£o
df3 = df2.select("name", "salary")       # Transforma√ß√£o

# Agora Spark executa TUDO de uma vez (otimizado)
df3.show()  # A√ß√£o - trigger execution
```

### Transforma√ß√µes (Lazy)

**Narrow Transformations** (sem shuffle):
```python
# map, filter, select - cada parti√ß√£o processa independente
df.select("name", "age")
df.filter(col("age") > 25)
df.withColumn("age_plus_10", col("age") + 10)
```

**Wide Transformations** (com shuffle):
```python
# groupBy, join - requer redistribuir dados entre parti√ß√µes
df.groupBy("role").count()
df1.join(df2, "id")
df.orderBy("salary")
```

### A√ß√µes (Eager - executam imediatamente)

```python
# Coletar dados para driver
df.collect()           # Retorna lista de Rows (CUIDADO: traz tudo para mem√≥ria)
df.take(5)             # Primeiros 5 rows
df.first()             # Primeiro row
df.head(3)             # Primeiros 3 rows

# Mostrar dados
df.show()              # Mostra 20 rows
df.show(100, False)    # 100 rows, sem truncar strings

# Contar
df.count()             # N√∫mero de rows
df.distinct().count()  # Rows √∫nicos

# Salvar
df.write.parquet("/path/to/output")
df.write.mode("overwrite").csv("/path/to/csv")

# Iterar (local)
for row in df.collect():
    print(row.name, row.age)
```

---

## Spark SQL

### Ler Dados

```python
# CSV
df = spark.read.csv("data.csv", header=True, inferSchema=True)

# Parquet (formato colunar otimizado)
df = spark.read.parquet("data.parquet")

# JSON
df = spark.read.json("data.json")

# JDBC (databases)
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:postgresql://localhost:5432/mydb") \
    .option("dbtable", "users") \
    .option("user", "admin") \
    .option("password", "secret") \
    .load()

# Hive table
df = spark.table("warehouse.users")

# Op√ß√µes avan√ßadas
df = spark.read \
    .option("header", "true") \
    .option("inferSchema", "true") \
    .option("delimiter", "|") \
    .option("quote", '"') \
    .option("escape", "\\") \
    .csv("data.csv")
```

### Opera√ß√µes Comuns

```python
from pyspark.sql.functions import *

# SELECT
df.select("name", "age")
df.select(col("name"), (col("salary") * 1.1).alias("new_salary"))

# WHERE / FILTER
df.filter(col("age") > 30)
df.where((col("age") > 25) & (col("role") == "Engineer"))

# GROUP BY
df.groupBy("role").agg(
    count("*").alias("count"),
    avg("salary").alias("avg_salary"),
    max("salary").alias("max_salary")
)

# ORDER BY
df.orderBy(col("salary").desc())
df.orderBy("age", "name")

# JOIN
df1.join(df2, df1.id == df2.user_id, "inner")  # inner, left, right, outer

# UNION
df1.union(df2)  # Mesmas colunas

# DISTINCT
df.select("role").distinct()

# DROP DUPLICATES
df.dropDuplicates(["name", "age"])

# WITH COLUMN
df.withColumn("age_category", 
    when(col("age") < 30, "Young")
    .when(col("age") < 50, "Middle")
    .otherwise("Senior")
)

# DROP COLUMN
df.drop("temporary_col")

# RENAME COLUMN
df.withColumnRenamed("old_name", "new_name")
```

### Window Functions

```python
from pyspark.sql.window import Window

# Definir window
window_spec = Window.partitionBy("role").orderBy(col("salary").desc())

# Ranking
df.withColumn("rank", rank().over(window_spec)) \
  .withColumn("dense_rank", dense_rank().over(window_spec)) \
  .withColumn("row_number", row_number().over(window_spec)) \
  .show()

# Agrega√ß√µes em janela
window_agg = Window.partitionBy("role")

df.withColumn("avg_salary_by_role", avg("salary").over(window_agg)) \
  .withColumn("max_salary_by_role", max("salary").over(window_agg)) \
  .show()

# Lag / Lead
window_order = Window.partitionBy("role").orderBy("hire_date")

df.withColumn("previous_salary", lag("salary", 1).over(window_order)) \
  .withColumn("next_salary", lead("salary", 1).over(window_order)) \
  .show()

# Running total
df.withColumn("running_total", 
    sum("salary").over(Window.partitionBy("role").orderBy("hire_date")
                       .rowsBetween(Window.unboundedPreceding, Window.currentRow))
).show()
```

### SQL Queries

```python
# Registrar DataFrame como temp view
df.createOrReplaceTempView("employees")

# SQL Query
result = spark.sql("""
    SELECT 
        role,
        COUNT(*) as count,
        AVG(salary) as avg_salary,
        PERCENTILE_APPROX(salary, 0.5) as median_salary
    FROM employees
    WHERE age > 25
    GROUP BY role
    HAVING AVG(salary) > 100000
    ORDER BY avg_salary DESC
""")

result.show()

# CTEs (Common Table Expressions)
spark.sql("""
    WITH high_earners AS (
        SELECT * FROM employees WHERE salary > 100000
    ),
    role_stats AS (
        SELECT role, AVG(salary) as avg_sal FROM high_earners GROUP BY role
    )
    SELECT * FROM role_stats WHERE avg_sal > 110000
""").show()
```

---

## Otimiza√ß√µes e Best Practices

### 1. Particionamento

```python
# Repartition (full shuffle - caro)
df_repart = df.repartition(10)  # 10 parti√ß√µes
df_repart = df.repartition(10, "role")  # Particionar por coluna

# Coalesce (reduzir parti√ß√µes SEM shuffle)
df_coal = df.coalesce(2)  # Reduz para 2 parti√ß√µes (mais eficiente)

# Verificar n√∫mero de parti√ß√µes
df.rdd.getNumPartitions()

# Salvar com particionamento
df.write.partitionBy("year", "month").parquet("/data/partitioned")
```

**Regra de ouro:** 1 parti√ß√£o = 1 core. Parti√ß√µes devem ter ~128MB-1GB cada.

### 2. Cache / Persist

```python
# Cache in-memory (padr√£o: MEMORY_AND_DISK)
df.cache()

# Persist com storage level customizado
from pyspark import StorageLevel
df.persist(StorageLevel.MEMORY_ONLY)
df.persist(StorageLevel.MEMORY_AND_DISK_SER)  # Serializado (menos mem√≥ria)

# Usar cache quando:
# - DataFrame √© reutilizado m√∫ltiplas vezes
# - Transforma√ß√µes custosas antes do cache

df_cached = df.filter(col("age") > 30).cache()
df_cached.count()  # Executa e cacheia
df_cached.groupBy("role").count().show()  # Usa cache
df_cached.select("name").show()  # Usa cache

# Liberar cache
df.unpersist()
```

### 3. Broadcast Join

Para joins com tabela pequena (<10MB):

```python
from pyspark.sql.functions import broadcast

# Join normal (shuffle dos dois lados)
df_large.join(df_small, "id")

# Broadcast join (df_small enviado para todos os executors)
df_large.join(broadcast(df_small), "id")

# Spark decide automaticamente se < spark.sql.autoBroadcastJoinThreshold (10MB)
```

### 4. Evitar Shuffles Desnecess√°rios

```python
# ‚ùå Ruim: m√∫ltiplos shuffles
df.groupBy("role").count() \
  .filter(col("count") > 5) \
  .orderBy("count")

# ‚úÖ Bom: filter antes do groupBy
df.filter(...)  # Reduz dados antes do shuffle
  .groupBy("role").count() \
  .orderBy("count")
```

### 5. Usar Catalyst Optimizer

```python
# DataFrame usa Catalyst optimizer automaticamente
df.filter(col("age") > 30).select("name")  # Otimizado

# RDD N√ÉO usa Catalyst
rdd.filter(lambda x: x[1] > 30).map(lambda x: x[0])  # N√£o otimizado

# Ver query plan
df.explain(True)  # Mostra: Parsed, Analyzed, Optimized, Physical plan
```

### 6. Evitar UDFs (User Defined Functions)

```python
# ‚ùå Lento: UDF em Python (serializa√ß√£o Python <-> JVM)
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

def age_category_udf(age):
    if age < 30: return 1
    elif age < 50: return 2
    else: return 3

age_cat = udf(age_category_udf, IntegerType())
df.withColumn("category", age_cat(col("age")))

# ‚úÖ R√°pido: usar fun√ß√µes nativas
df.withColumn("category",
    when(col("age") < 30, 1)
    .when(col("age") < 50, 2)
    .otherwise(3)
)

# Se UDF √© necess√°ria, use Pandas UDF (vetorizado)
from pyspark.sql.functions import pandas_udf
import pandas as pd

@pandas_udf(IntegerType())
def age_category_pandas(ages: pd.Series) -> pd.Series:
    return pd.cut(ages, bins=[0, 30, 50, 100], labels=[1, 2, 3])

df.withColumn("category", age_category_pandas(col("age")))
```

---

## Spark Streaming

### Structured Streaming

```python
# Ler stream de arquivos
stream_df = spark.readStream \
    .format("csv") \
    .option("header", "true") \
    .schema(schema) \
    .load("/data/streaming/input/")

# Transforma√ß√µes (mesmas APIs do DataFrame)
result = stream_df \
    .filter(col("value") > 100) \
    .groupBy("category").count()

# Escrever stream
query = result.writeStream \
    .outputMode("complete")  # complete, append, update
    .format("console") \
    .start()

query.awaitTermination()

# Stream de Kafka
kafka_df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "topic1") \
    .load()

# Processar mensagens
messages = kafka_df.selectExpr("CAST(value AS STRING)")

# Watermark para late data
stream_df.withWatermark("timestamp", "10 minutes") \
    .groupBy(window(col("timestamp"), "5 minutes"), "user_id") \
    .count()
```

### Triggers

```python
# Continuous (low latency ~1ms)
.trigger(continuous="1 second")

# Micro-batch (default)
.trigger(processingTime="5 seconds")

# One-time (processa dados dispon√≠veis e para)
.trigger(once=True)

# Available-now (processa tudo que est√° dispon√≠vel)
.trigger(availableNow=True)
```

---

## üéØ Checklist de Performance

- ‚úÖ Use DataFrame ao inv√©s de RDD
- ‚úÖ Particione dados apropriadamente (128MB-1GB por parti√ß√£o)
- ‚úÖ Cache DataFrames reutilizados
- ‚úÖ Use broadcast join para tabelas pequenas
- ‚úÖ Evite UDFs Python (use fun√ß√µes nativas ou Pandas UDF)
- ‚úÖ Use formatos colunares (Parquet, ORC)
- ‚úÖ Filter dados cedo (pushdown predicates)
- ‚úÖ Evite shuffles desnecess√°rios
- ‚úÖ Monitore Spark UI para identificar bottlenecks

---

## üìö Refer√™ncias

- [Spark Documentation](https://spark.apache.org/docs/latest/)
- [Spark: The Definitive Guide](https://www.oreilly.com/library/view/spark-the-definitive/9781491912201/)
- [High Performance Spark](https://www.oreilly.com/library/view/high-performance-spark/9781491943199/)

---

**Pr√≥ximo:** [02-spark-performance-tuning.md](./02-spark-performance-tuning.md)
