# Arquiteturas de Sistemas de Dados

## 1. OLTP vs OLAP

### OLTP (Online Transaction Processing)

**Caracter√≠sticas:**
- **Foco**: Transa√ß√µes r√°pidas e frequentes
- **Opera√ß√µes**: INSERT, UPDATE, DELETE
- **Dados**: Atuais, operacionais
- **Normaliza√ß√£o**: Alta (3NF ou superior)
- **Usu√°rios**: Milhares simult√¢neos
- **Tamanho**: GB a TB
- **Performance**: Milissegundos

**Exemplos de uso:**
- Sistemas banc√°rios
- E-commerce (carrinho, checkout)
- CRM (gest√£o de clientes)
- ERP (planejamento de recursos)

**Bancos OLTP populares:**
```
PostgreSQL, MySQL, Oracle, SQL Server, MongoDB
```

**Exemplo pr√°tico:**
```sql
-- Transa√ß√£o de pedido (OLTP)
BEGIN TRANSACTION;

-- 1. Criar pedido
INSERT INTO orders (user_id, total_amount, status, created_at)
VALUES (12345, 299.99, 'pending', NOW())
RETURNING order_id INTO @order_id;

-- 2. Adicionar itens
INSERT INTO order_items (order_id, product_id, quantity, price)
VALUES 
    (@order_id, 101, 2, 99.99),
    (@order_id, 205, 1, 100.01);

-- 3. Atualizar invent√°rio
UPDATE inventory 
SET quantity = quantity - 2,
    last_updated = NOW()
WHERE product_id = 101;

UPDATE inventory 
SET quantity = quantity - 1,
    last_updated = NOW()
WHERE product_id = 205;

-- 4. Registrar pagamento
INSERT INTO payments (order_id, amount, payment_method, status)
VALUES (@order_id, 299.99, 'credit_card', 'pending');

COMMIT;

-- Caracter√≠sticas OLTP desta transa√ß√£o:
-- ‚úÖ M√∫ltiplas tabelas (normalizado)
-- ‚úÖ ACID compliance (atomicidade)
-- ‚úÖ Escrita intensiva
-- ‚úÖ Resposta em < 100ms
-- ‚úÖ Dados operacionais atuais
```

### OLAP (Online Analytical Processing)

**Caracter√≠sticas:**
- **Foco**: An√°lises complexas e reports
- **Opera√ß√µes**: SELECT com agrega√ß√µes
- **Dados**: Hist√≥ricos, consolidados
- **Normaliza√ß√£o**: Baixa (Star/Snowflake Schema)
- **Usu√°rios**: Dezenas a centenas
- **Tamanho**: TB a PB
- **Performance**: Segundos a minutos

**Exemplos de uso:**
- Business Intelligence
- Data Analytics
- Relat√≥rios gerenciais
- Dashboards executivos

**Bancos OLAP populares:**
```
Snowflake, Redshift, BigQuery, Synapse Analytics, ClickHouse
```

**Exemplo pr√°tico:**
```sql
-- An√°lise de vendas (OLAP)
WITH monthly_sales AS (
    SELECT
        DATE_TRUNC('month', order_date) as month,
        product_category,
        region,
        SUM(total_amount) as total_sales,
        COUNT(DISTINCT customer_id) as unique_customers,
        COUNT(*) as num_orders,
        AVG(total_amount) as avg_order_value
    FROM fact_sales f
    JOIN dim_product p ON f.product_id = p.product_id
    JOIN dim_location l ON f.location_id = l.location_id
    WHERE order_date >= '2023-01-01'
    GROUP BY 1, 2, 3
),
growth_calc AS (
    SELECT
        month,
        product_category,
        region,
        total_sales,
        LAG(total_sales) OVER (
            PARTITION BY product_category, region 
            ORDER BY month
        ) as prev_month_sales,
        (total_sales - LAG(total_sales) OVER (
            PARTITION BY product_category, region 
            ORDER BY month
        )) / NULLIF(LAG(total_sales) OVER (
            PARTITION BY product_category, region 
            ORDER BY month
        ), 0) * 100 as growth_pct
    FROM monthly_sales
)
SELECT
    month,
    product_category,
    region,
    total_sales,
    prev_month_sales,
    growth_pct,
    CASE
        WHEN growth_pct > 20 THEN 'üöÄ Alto Crescimento'
        WHEN growth_pct > 0 THEN 'üìà Crescimento'
        WHEN growth_pct = 0 THEN '‚û°Ô∏è Est√°vel'
        ELSE 'üìâ Decl√≠nio'
    END as trend
FROM growth_calc
WHERE month >= '2024-01-01'
ORDER BY month DESC, total_sales DESC;

-- Caracter√≠sticas OLAP desta query:
-- ‚úÖ Leitura pesada (sem writes)
-- ‚úÖ Agrega√ß√µes complexas (SUM, AVG, COUNT)
-- ‚úÖ Window functions (LAG, PARTITION BY)
-- ‚úÖ M√∫ltiplos JOINs com fact/dimension tables
-- ‚úÖ An√°lise de tend√™ncias temporais
-- ‚úÖ Pode levar segundos para executar
-- ‚úÖ Processa milh√µes/bilh√µes de linhas
```

**Compara√ß√£o lado a lado:**

| Aspecto | OLTP | OLAP |
|---------|------|------|
| **Prop√≥sito** | Opera√ß√µes di√°rias | An√°lises e decis√µes |
| **Opera√ß√µes** | INSERT, UPDATE, DELETE | SELECT complexos |
| **Queries** | Simples, r√°pidas | Complexas, lentas |
| **Dados** | Atuais | Hist√≥ricos |
| **Volume por query** | Poucos registros | Milh√µes de registros |
| **Normaliza√ß√£o** | Alta (3NF) | Baixa (Star Schema) |
| **√çndices** | Muitos (B-Tree) | Poucos (Columnar) |
| **Throughput** | Alto (1000s TPS) | Baixo (10s queries/sec) |
| **Lat√™ncia** | < 100ms | Segundos a minutos |
| **Backup** | Frequente (hourly) | Menos frequente (daily) |
| **Exemplos** | PostgreSQL, MySQL | Snowflake, Redshift |

---

## 2. Data Warehouse

### Defini√ß√£o e Prop√≥sito

Um **Data Warehouse (DW)** √© um reposit√≥rio centralizado de dados integrados de m√∫ltiplas fontes, otimizado para an√°lise e relat√≥rios.

**Caracter√≠sticas principais:**
- **Subject-oriented**: Organizado por assunto (vendas, clientes, produtos)
- **Integrated**: Dados de m√∫ltiplas fontes consolidados
- **Non-volatile**: Dados hist√≥ricos n√£o s√£o alterados
- **Time-variant**: Mant√©m hist√≥rico temporal

### Arquitetura Kimball (Bottom-up)

```
Sources ‚Üí ETL ‚Üí Data Marts ‚Üí Business Intelligence
(OLTP)         (Star Schema)  (Reports, Dashboards)
```

**Componentes:**
1. **Staging Area**: Dados brutos tempor√°rios
2. **Data Marts**: √Åreas departamentais (Vendas, Marketing, Finan√ßas)
3. **Presentation Layer**: Star/Snowflake Schema
4. **BI Layer**: Tableau, PowerBI, Looker

**Vantagens:**
- ‚úÖ R√°pida implementa√ß√£o (bottom-up)
- ‚úÖ ROI mais r√°pido
- ‚úÖ Flex√≠vel para mudan√ßas
- ‚úÖ F√°cil de entender (dimensional)

### Arquitetura Inmon (Top-down)

```
Sources ‚Üí ETL ‚Üí Enterprise DW ‚Üí Data Marts ‚Üí BI
(OLTP)         (3NF Normalized)  (Denormalized)
```

**Componentes:**
1. **Staging Area**: Dados brutos
2. **Enterprise Data Warehouse**: Dados normalizados (3NF)
3. **Data Marts**: Views desnormalizadas
4. **BI Layer**: Relat√≥rios

**Vantagens:**
- ‚úÖ √önica fonte da verdade
- ‚úÖ Consist√™ncia de dados
- ‚úÖ Escal√°vel para enterprise
- ‚úÖ Menos redund√¢ncia

**Desvantagens:**
- ‚ùå Implementa√ß√£o longa (anos)
- ‚ùå Custo inicial alto
- ‚ùå Complexidade maior

### Star Schema (Kimball)

```
       DIM_TEMPO
           |
DIM_CLIENTE - FACT_VENDAS - DIM_PRODUTO
           |
       DIM_LOJA
```

**Fact Table (Fatos):**
- M√©tricas num√©ricas (vendas, quantidade, lucro)
- Foreign keys para dimens√µes
- Granularidade (n√≠vel de detalhe)

```sql
CREATE TABLE fact_vendas (
    venda_id BIGINT PRIMARY KEY,
    data_id INT NOT NULL,          -- FK para dim_tempo
    cliente_id INT NOT NULL,        -- FK para dim_cliente
    produto_id INT NOT NULL,        -- FK para dim_produto
    loja_id INT NOT NULL,           -- FK para dim_loja
    
    -- M√©tricas
    quantidade INT,
    valor_unitario DECIMAL(10,2),
    valor_total DECIMAL(10,2),
    custo DECIMAL(10,2),
    lucro DECIMAL(10,2),
    desconto DECIMAL(10,2),
    
    FOREIGN KEY (data_id) REFERENCES dim_tempo(data_id),
    FOREIGN KEY (cliente_id) REFERENCES dim_cliente(cliente_id),
    FOREIGN KEY (produto_id) REFERENCES dim_produto(produto_id),
    FOREIGN KEY (loja_id) REFERENCES dim_loja(loja_id)
);
```

**Dimension Tables (Dimens√µes):**
- Contexto descritivo
- Atributos textuais
- Surrogate keys (SK) e Natural keys (NK)

```sql
CREATE TABLE dim_produto (
    produto_id INT PRIMARY KEY,           -- Surrogate Key
    produto_nk VARCHAR(50) NOT NULL,      -- Natural Key (SKU)
    nome VARCHAR(200),
    descricao TEXT,
    categoria VARCHAR(100),
    subcategoria VARCHAR(100),
    marca VARCHAR(100),
    preco_sugerido DECIMAL(10,2),
    
    -- Metadados (SCD Type 2)
    valido_de TIMESTAMP,
    valido_ate TIMESTAMP,
    eh_atual BOOLEAN DEFAULT TRUE,
    versao INT DEFAULT 1
);

CREATE TABLE dim_tempo (
    data_id INT PRIMARY KEY,              -- YYYYMMDD
    data DATE NOT NULL,
    ano INT,
    trimestre INT,
    mes INT,
    semana INT,
    dia INT,
    dia_semana INT,
    nome_dia_semana VARCHAR(20),
    nome_mes VARCHAR(20),
    eh_fim_de_semana BOOLEAN,
    eh_feriado BOOLEAN,
    nome_feriado VARCHAR(100)
);
```

### Slowly Changing Dimensions (SCD)

**Type 0: Retain Original**
- Nunca muda
- Ex: Data de nascimento

**Type 1: Overwrite**
```sql
-- Antes
UPDATE dim_cliente 
SET endereco = 'Rua Nova, 123',
    cidade = 'S√£o Paulo'
WHERE cliente_id = 1001;

-- Depois: Hist√≥rico perdido
```

**Type 2: Add New Row (mais comum)**
```sql
-- Desativar registro antigo
UPDATE dim_cliente 
SET valido_ate = CURRENT_TIMESTAMP,
    eh_atual = FALSE
WHERE cliente_id = 1001 AND eh_atual = TRUE;

-- Inserir novo registro
INSERT INTO dim_cliente (
    cliente_nk, nome, endereco, cidade,
    valido_de, valido_ate, eh_atual, versao
)
VALUES (
    'C1001', 'Jo√£o Silva', 'Rua Nova, 123', 'S√£o Paulo',
    CURRENT_TIMESTAMP, NULL, TRUE, 2
);

-- Resultado: Hist√≥rico completo mantido
-- Vers√£o 1: Endere√ßo antigo (2023-01-01 a 2024-01-15)
-- Vers√£o 2: Endere√ßo novo (2024-01-15 at√© presente)
```

**Type 3: Add New Column**
```sql
ALTER TABLE dim_cliente 
ADD COLUMN endereco_anterior VARCHAR(200),
ADD COLUMN data_mudanca_endereco TIMESTAMP;

UPDATE dim_cliente 
SET endereco_anterior = endereco,
    endereco = 'Rua Nova, 123',
    data_mudanca_endereco = CURRENT_TIMESTAMP
WHERE cliente_id = 1001;

-- Mant√©m apenas √∫ltima mudan√ßa
```

---

## 3. Data Lake

### Conceito

**Data Lake** = Reposit√≥rio centralizado de dados RAW em formato nativo (schema-on-read).

**Diferen√ßas do DW:**
- **Schema**: Schema-on-read (aplicado na leitura) vs Schema-on-write
- **Dados**: Todos os tipos vs Apenas estruturados
- **Processamento**: ELT vs ETL
- **Usu√°rios**: Data Scientists, Engineers vs Business Analysts

### Arquitetura em Camadas (Medallion)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  BRONZE LAYER (Raw)                                     ‚îÇ
‚îÇ  - Dados brutos, sem transforma√ß√£o                      ‚îÇ
‚îÇ  - Formato original (JSON, CSV, Parquet)                ‚îÇ
‚îÇ  - Append-only (imut√°vel)                              ‚îÇ
‚îÇ  - Particionado por data de ingest√£o                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SILVER LAYER (Refined)                                 ‚îÇ
‚îÇ  - Dados limpos e validados                            ‚îÇ
‚îÇ  - Schema enforced                                      ‚îÇ
‚îÇ  - Deduplica√ß√£o                                        ‚îÇ
‚îÇ  - Tipo de dados corretos                              ‚îÇ
‚îÇ  - Particionado por l√≥gica de neg√≥cio                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  GOLD LAYER (Curated)                                   ‚îÇ
‚îÇ  - Dados agregados e prontos para consumo              ‚îÇ
‚îÇ  - Features para ML                                     ‚îÇ
‚îÇ  - Reports e dashboards                                 ‚îÇ
‚îÇ  - Star schema (opcional)                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Exemplo pr√°tico com PySpark

**Bronze ‚Üí Silver:**
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

spark = SparkSession.builder.appName("DataLake").getOrCreate()

# BRONZE: Ler dados brutos
bronze_df = spark.read \
    .format("json") \
    .option("multiline", "true") \
    .load("s3://datalake/bronze/events/")

bronze_df.write \
    .format("parquet") \
    .partitionBy("year", "month", "day") \
    .mode("append") \
    .save("s3://datalake/bronze/events_parquet/")

# SILVER: Limpar e validar
silver_df = bronze_df \
    .filter(col("user_id").isNotNull()) \
    .filter(col("timestamp").isNotNull()) \
    .withColumn("timestamp", to_timestamp(col("timestamp"))) \
    .withColumn("date", to_date(col("timestamp"))) \
    .dropDuplicates(["event_id"]) \
    .withColumn("processed_at", current_timestamp())

# Data quality checks
from pyspark.sql.functions import count, countDistinct

quality_metrics = silver_df.agg(
    count("*").alias("total_records"),
    countDistinct("user_id").alias("unique_users"),
    countDistinct("event_id").alias("unique_events"),
    sum(when(col("user_id").isNull(), 1).otherwise(0)).alias("null_user_ids")
).collect()[0]

assert quality_metrics["null_user_ids"] == 0, "Null user_ids found!"

silver_df.write \
    .format("delta") \
    .partitionBy("date") \
    .mode("overwrite") \
    .save("s3://datalake/silver/events/")

# GOLD: Agregar para analytics
gold_df = silver_df \
    .groupBy("date", "user_id", "event_type") \
    .agg(
        count("*").alias("event_count"),
        countDistinct("session_id").alias("unique_sessions")
    )

gold_df.write \
    .format("delta") \
    .partitionBy("date") \
    .mode("overwrite") \
    .save("s3://datalake/gold/user_daily_events/")
```

### Data Lakehouse

**Conceito**: Combina melhores pr√°ticas de Data Lake + Data Warehouse

**Tecnologias:**
- **Delta Lake** (Databricks)
- **Apache Iceberg** (Netflix)
- **Apache Hudi** (Uber)

**Caracter√≠sticas:**
- ‚úÖ ACID transactions
- ‚úÖ Time travel
- ‚úÖ Schema enforcement e evolution
- ‚úÖ Performance de DW com flexibilidade de Data Lake
- ‚úÖ Unified batch + streaming

**Exemplo Delta Lake:**
```python
from delta import *

# Escrever com Delta Lake
df.write \
    .format("delta") \
    .mode("overwrite") \
    .save("/data/delta/sales")

# Ler com time travel
df_yesterday = spark.read \
    .format("delta") \
    .option("versionAsOf", 1) \
    .load("/data/delta/sales")

df_last_week = spark.read \
    .format("delta") \
    .option("timestampAsOf", "2024-01-01") \
    .load("/data/delta/sales")

# MERGE (upsert)
from delta.tables import *

deltaTable = DeltaTable.forPath(spark, "/data/delta/sales")

deltaTable.alias("target").merge(
    source_df.alias("source"),
    "target.sale_id = source.sale_id"
).whenMatchedUpdate(set = {
    "quantity": "source.quantity",
    "updated_at": "current_timestamp()"
}).whenNotMatchedInsert(values = {
    "sale_id": "source.sale_id",
    "quantity": "source.quantity",
    "created_at": "current_timestamp()"
}).execute()

# Vacuum (limpar vers√µes antigas)
deltaTable.vacuum(retentionHours=168)  # 7 dias
```

---

## Resumo Comparativo

| Aspecto | OLTP | Data Warehouse | Data Lake | Data Lakehouse |
|---------|------|----------------|-----------|----------------|
| **Dados** | Atuais | Hist√≥ricos | Raw | Raw + Curados |
| **Schema** | Definido | Definido | Flex√≠vel | Flex√≠vel + Enforced |
| **Formato** | Tabelas | Star/Snow | Arquivos | Delta/Iceberg |
| **Usu√°rios** | Apps | Analistas | Cientistas | Todos |
| **Custo** | $$$ | $$$$ | $ | $$ |
| **Performance** | Alta | Alta | M√©dia | Alta |
| **Flexibilidade** | Baixa | Baixa | Alta | Alta |
| **ACID** | ‚úÖ | ‚úÖ | ‚ùå | ‚úÖ |
| **Exemplos** | PostgreSQL | Snowflake | S3 | Delta Lake |

---

**Pr√≥ximo**: [02-sql-avancado.md](./02-sql-avancado.md)
