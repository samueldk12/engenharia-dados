<!DOCTYPE html>
<html lang="pt-BR">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GCP Professional Data Engineer - Certification Guide</title>
    <link rel="stylesheet" href="../assets/css/style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
</head>
<body>
    <nav class="navbar">
        <div class="container">
            <div class="nav-brand">
                <a href="../index.html" style="text-decoration: none; color: inherit;">
                    <i class="fas fa-database"></i>
                    <span>Data Engineering Roadmap</span>
                </a>
            </div>
            <ul class="nav-links">
                <li><a href="../index.html">In√≠cio</a></li>
                <li><a href="../index.html#certifications">Certifica√ß√µes</a></li>
                <li><a href="https://github.com/samueldk12/engenharia-dados">GitHub</a></li>
            </ul>
        </div>
    </nav>

    <section class="cert-detail">
        <div class="container">
            <div class="cert-header">
                <div class="cert-provider gcp">GCP</div>
                <h1>Google Cloud Professional Data Engineer</h1>
                <div class="cert-meta">
                    <span><i class="fas fa-dollar-sign"></i> $200 USD</span>
                    <span><i class="fas fa-clock"></i> 120 minutos</span>
                    <span><i class="fas fa-question-circle"></i> 50-60 quest√µes</span>
                    <span><i class="fas fa-chart-line"></i> ROI: +$15-30K/ano</span>
                </div>
            </div>

            <div class="cert-content">
                <section>
                    <h2>üìã Vis√£o Geral</h2>
                    <p>
                        A certifica√ß√£o GCP Professional Data Engineer valida habilidade de design, constru√ß√£o, 
                        opera√ß√£o e seguran√ßa de sistemas de processamento de dados no Google Cloud Platform.
                    </p>
                    <p><strong>N√≠vel:</strong> Professional (Avan√ßado)</p>
                    <p><strong>Validade:</strong> 2 anos</p>
                    <p><strong>Score m√≠nimo:</strong> N√£o divulgado (~70%)</p>
                    <p><strong>Formato:</strong> Multiple choice + Multiple select + Case studies</p>
                </section>

                <section>
                    <h2>üìö Dom√≠nios do Exame</h2>
                    <div class="domains-grid">
                        <div class="domain-card">
                            <div class="domain-header">
                                <h3>1. Design Data Processing Systems (22%)</h3>
                            </div>
                            <ul>
                                <li>Designing flexible data representations</li>
                                <li>Designing data pipelines</li>
                                <li>Designing data processing infrastructure</li>
                                <li>Migrating data warehousing/processing</li>
                            </ul>
                        </div>

                        <div class="domain-card">
                            <div class="domain-header">
                                <h3>2. Ingestion and Processing (25%)</h3>
                            </div>
                            <ul>
                                <li>Pub/Sub for messaging</li>
                                <li>Dataflow (Apache Beam)</li>
                                <li>Dataproc (Spark/Hadoop)</li>
                                <li>Cloud Functions</li>
                                <li>Batch vs Streaming</li>
                            </ul>
                        </div>

                        <div class="domain-card">
                            <div class="domain-header">
                                <h3>3. Storage (20%)</h3>
                            </div>
                            <ul>
                                <li>Cloud Storage (Data Lake)</li>
                                <li>BigQuery (Data Warehouse)</li>
                                <li>Cloud SQL, Cloud Spanner</li>
                                <li>Bigtable, Firestore</li>
                                <li>Storage optimization</li>
                            </ul>
                        </div>

                        <div class="domain-card">
                            <div class="domain-header">
                                <h3>4. ML and Analytics (17%)</h3>
                            </div>
                            <ul>
                                <li>BigQuery ML</li>
                                <li>Vertex AI</li>
                                <li>AutoML</li>
                                <li>Pre-trained APIs</li>
                                <li>Data Studio/Looker</li>
                            </ul>
                        </div>

                        <div class="domain-card">
                            <div class="domain-header">
                                <h3>5. Reliability & Security (16%)</h3>
                            </div>
                            <ul>
                                <li>IAM and access control</li>
                                <li>Data encryption</li>
                                <li>Monitoring (Cloud Monitoring)</li>
                                <li>Logging (Cloud Logging)</li>
                                <li>Disaster recovery</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üìñ Plano de Estudos (8 semanas)</h2>
                    
                    <div class="study-plan">
                        <div class="week">
                            <h3>Semana 1-2: Storage & BigQuery</h3>
                            <ul>
                                <li>Cloud Storage (buckets, classes, lifecycle)</li>
                                <li>BigQuery architecture e best practices</li>
                                <li>Partitioning e Clustering</li>
                                <li>BigQuery ML basics</li>
                                <li><strong>Labs:</strong> Create partitioned tables, run ML models</li>
                            </ul>
                        </div>

                        <div class="week">
                            <h3>Semana 3-4: Data Processing</h3>
                            <ul>
                                <li>Dataflow (Apache Beam) - batch e streaming</li>
                                <li>Dataproc (Spark/Hadoop clusters)</li>
                                <li>Pub/Sub for event streaming</li>
                                <li>Cloud Functions para event-driven</li>
                                <li><strong>Labs:</strong> Build Dataflow pipeline, Spark job</li>
                            </ul>
                        </div>

                        <div class="week">
                            <h3>Semana 5-6: ML & Analytics</h3>
                            <ul>
                                <li>BigQuery ML (CREATE MODEL)</li>
                                <li>Vertex AI for custom models</li>
                                <li>AutoML (no-code ML)</li>
                                <li>Data Studio dashboards</li>
                                <li><strong>Labs:</strong> Train and deploy ML models</li>
                            </ul>
                        </div>

                        <div class="week">
                            <h3>Semana 7-8: Security & Case Studies</h3>
                            <ul>
                                <li>IAM roles e permissions</li>
                                <li>VPC Service Controls</li>
                                <li>Encryption (CMEK, CSEK)</li>
                                <li>Study case studies (3-4 fornecidos)</li>
                                <li>Practice exams (3+)</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üí° Dicas para o Exame</h2>
                    <ul>
                        <li><strong>Case Studies:</strong> Leia os 3-4 case studies ANTES do exame (dispon√≠veis no site)</li>
                        <li><strong>BigQuery:</strong> Partitioning por data + Clustering = otimiza√ß√£o mais comum</li>
                        <li><strong>Dataflow vs Dataproc:</strong> Dataflow = serverless, Dataproc = managed clusters</li>
                        <li><strong>Pub/Sub:</strong> Entenda at-least-once delivery e ordering guarantees</li>
                        <li><strong>Storage Classes:</strong> Standard, Nearline, Coldline, Archive - use cases</li>
                        <li><strong>IAM:</strong> Princ√≠pio do menor privil√©gio - use predefined roles quando poss√≠vel</li>
                        <li><strong>Hands-on:</strong> Use o Free Tier do GCP para praticar - fundamental!</li>
                    </ul>
                </section>

                <section>
                    <h2>üíª Exemplos de C√≥digo</h2>
                    
                    <h3>1. BigQuery - Partitioning e Clustering</h3>
                    <pre><code class="language-sql">
-- Create partitioned and clustered table
CREATE TABLE `project.dataset.sales`
(
    order_id INT64,
    order_date DATE,
    customer_id INT64,
    product_id INT64,
    amount NUMERIC
)
PARTITION BY order_date
CLUSTER BY customer_id, product_id
OPTIONS(
    partition_expiration_days=365,
    require_partition_filter=true
);

-- Insert data
INSERT INTO `project.dataset.sales`
SELECT * FROM `project.dataset.sales_raw`
WHERE order_date >= '2024-01-01';

-- Query with partition pruning (very efficient)
SELECT 
    customer_id,
    SUM(amount) as total_sales
FROM `project.dataset.sales`
WHERE order_date BETWEEN '2024-01-01' AND '2024-01-31'
GROUP BY customer_id;
                    </code></pre>

                    <h3>2. BigQuery ML - Train Model</h3>
                    <pre><code class="language-sql">
-- Create logistic regression model for churn prediction
CREATE OR REPLACE MODEL `project.dataset.churn_model`
OPTIONS(
    model_type='LOGISTIC_REG',
    input_label_cols=['churned'],
    auto_class_weights=true
) AS
SELECT
    customer_age,
    total_purchases,
    avg_purchase_amount,
    days_since_last_purchase,
    churned
FROM `project.dataset.customer_features`
WHERE split_field = 'TRAIN';

-- Evaluate model
SELECT
    roc_auc,
    precision,
    recall
FROM ML.EVALUATE(MODEL `project.dataset.churn_model`,
    (SELECT * FROM `project.dataset.customer_features` WHERE split_field = 'TEST')
);

-- Make predictions
SELECT
    customer_id,
    predicted_churned,
    predicted_churned_probs[OFFSET(1)].prob as churn_probability
FROM ML.PREDICT(MODEL `project.dataset.churn_model`,
    (SELECT * FROM `project.dataset.customers_current`)
)
WHERE predicted_churned_probs[OFFSET(1)].prob > 0.7
ORDER BY churn_probability DESC;
                    </code></pre>

                    <h3>3. Dataflow (Apache Beam) - Streaming Pipeline</h3>
                    <pre><code class="language-python">
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
from apache_beam.io.gcp.pubsub import ReadFromPubSub
from apache_beam.io.gcp.bigquery import WriteToBigQuery

class ParseMessage(beam.DoFn):
    def process(self, element):
        import json
        data = json.loads(element.decode('utf-8'))
        yield {
            'user_id': data['user_id'],
            'event_type': data['event_type'],
            'timestamp': data['timestamp'],
            'value': data.get('value', 0)
        }

class AggregateEvents(beam.DoFn):
    def process(self, element):
        key, values = element
        yield {
            'user_id': key,
            'event_count': len(values),
            'total_value': sum(v['value'] for v in values),
            'window_start': values[0]['timestamp']
        }

def run():
    options = PipelineOptions(
        project='my-project',
        streaming=True,
        runner='DataflowRunner',
        temp_location='gs://my-bucket/temp',
        region='us-central1'
    )
    
    with beam.Pipeline(options=options) as pipeline:
        (pipeline
            | 'Read from Pub/Sub' >> ReadFromPubSub(
                subscription='projects/my-project/subscriptions/events-sub'
            )
            | 'Parse JSON' >> beam.ParDo(ParseMessage())
            | 'Add Timestamp' >> beam.Map(
                lambda x: beam.window.TimestampedValue(x, x['timestamp'])
            )
            | 'Window' >> beam.WindowInto(
                beam.window.FixedWindows(60)  # 1-minute windows
            )
            | 'Key by User' >> beam.Map(lambda x: (x['user_id'], x))
            | 'Group by Key' >> beam.GroupByKey()
            | 'Aggregate' >> beam.ParDo(AggregateEvents())
            | 'Write to BigQuery' >> WriteToBigQuery(
                table='my-project:dataset.user_metrics',
                schema='user_id:STRING,event_count:INTEGER,total_value:FLOAT,window_start:TIMESTAMP',
                write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND
            )
        )

if __name__ == '__main__':
    run()
                    </code></pre>

                    <h3>4. Cloud Functions - Pub/Sub Trigger</h3>
                    <pre><code class="language-python">
import base64
import json
from google.cloud import bigquery

def process_pubsub_message(event, context):
    """
    Cloud Function triggered by Pub/Sub.
    Processes message and writes to BigQuery.
    """
    # Decode Pub/Sub message
    pubsub_message = base64.b64decode(event['data']).decode('utf-8')
    message_data = json.loads(pubsub_message)
    
    # Transform data
    row = {
        'user_id': message_data['user_id'],
        'event_type': message_data['event_type'],
        'timestamp': message_data['timestamp'],
        'processed_at': context.timestamp
    }
    
    # Write to BigQuery
    client = bigquery.Client()
    table_id = 'my-project.dataset.events'
    
    errors = client.insert_rows_json(table_id, [row])
    
    if errors:
        print(f"Errors: {errors}")
        raise Exception("Failed to insert row into BigQuery")
    
    print(f"Successfully processed event for user {message_data['user_id']}")
                    </code></pre>

                    <h3>5. IAM - Grant Permissions</h3>
                    <pre><code class="language-bash">
# Grant BigQuery Data Editor role to service account
gcloud projects add-iam-policy-binding my-project \
    --member="serviceAccount:data-engineer@my-project.iam.gserviceaccount.com" \
    --role="roles/bigquery.dataEditor"

# Grant Storage Object Viewer for specific bucket
gsutil iam ch serviceAccount:data-engineer@my-project.iam.gserviceaccount.com:objectViewer \
    gs://my-data-bucket

# Create custom role with specific permissions
gcloud iam roles create dataEngineerCustom \
    --project=my-project \
    --title="Custom Data Engineer" \
    --description="Custom role for data engineers" \
    --permissions=bigquery.tables.create,bigquery.tables.get,bigquery.jobs.create \
    --stage=GA
                    </code></pre>
                </section>

                <section>
                    <h2>üîë Principais Servi√ßos GCP</h2>
                    <div class="component-grid">
                        <div class="component-card">
                            <h3><i class="fas fa-database"></i> BigQuery</h3>
                            <ul>
                                <li>Serverless data warehouse</li>
                                <li>Peta-scale SQL analytics</li>
                                <li>Built-in ML (BigQuery ML)</li>
                                <li>Pay-per-query pricing</li>
                            </ul>
                        </div>

                        <div class="component-card">
                            <h3><i class="fas fa-stream"></i> Dataflow</h3>
                            <ul>
                                <li>Managed Apache Beam</li>
                                <li>Batch and streaming</li>
                                <li>Auto-scaling</li>
                                <li>Unified programming model</li>
                            </ul>
                        </div>

                        <div class="component-card">
                            <h3><i class="fas fa-fire"></i> Dataproc</h3>
                            <ul>
                                <li>Managed Spark/Hadoop</li>
                                <li>Fast cluster provisioning</li>
                                <li>Integration with GCS</li>
                                <li>Per-second billing</li>
                            </ul>
                        </div>

                        <div class="component-card">
                            <h3><i class="fas fa-broadcast-tower"></i> Pub/Sub</h3>
                            <ul>
                                <li>Global message bus</li>
                                <li>At-least-once delivery</li>
                                <li>Scales to millions msg/sec</li>
                                <li>Push and pull subscriptions</li>
                            </ul>
                        </div>

                        <div class="component-card">
                            <h3><i class="fas fa-hdd"></i> Cloud Storage</h3>
                            <ul>
                                <li>Object storage (Data Lake)</li>
                                <li>Multiple storage classes</li>
                                <li>Lifecycle management</li>
                                <li>Strong consistency</li>
                            </ul>
                        </div>

                        <div class="component-card">
                            <h3><i class="fas fa-brain"></i> Vertex AI</h3>
                            <ul>
                                <li>Unified ML platform</li>
                                <li>AutoML and custom models</li>
                                <li>Feature Store</li>
                                <li>Model deployment</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>üìö Recursos</h2>
                    <div class="resources-grid">
                        <div class="resource-card">
                            <h3>Cursos</h3>
                            <ul>
                                <li>Google Cloud Training (oficial)</li>
                                <li>Coursera - Data Engineering on GCP</li>
                                <li>A Cloud Guru</li>
                                <li>Linux Academy</li>
                            </ul>
                        </div>

                        <div class="resource-card">
                            <h3>Practice Exams</h3>
                            <ul>
                                <li>Google Official Practice Exam ($20)</li>
                                <li>Whizlabs</li>
                                <li>Udemy Practice Tests</li>
                            </ul>
                        </div>

                        <div class="resource-card">
                            <h3>Documenta√ß√£o</h3>
                            <ul>
                                <li>GCP Documentation</li>
                                <li>BigQuery Best Practices</li>
                                <li>Dataflow Templates</li>
                                <li>Architecture Framework</li>
                            </ul>
                        </div>

                        <div class="resource-card">
                            <h3>Case Studies</h3>
                            <ul>
                                <li>Flowlogistic (oficial)</li>
                                <li>MJTelco (oficial)</li>
                                <li>TerramEarth (oficial)</li>
                                <li>Leia ANTES do exame!</li>
                            </ul>
                        </div>
                    </div>
                </section>

                <section>
                    <h2>‚ö†Ô∏è Pegadinhas Comuns</h2>
                    <ul>
                        <li><strong>BigQuery Slots:</strong> Flat-rate vs on-demand pricing - entenda diferen√ßas</li>
                        <li><strong>Dataflow vs Dataproc:</strong> Use Dataflow para streaming, Dataproc para Spark jobs existentes</li>
                        <li><strong>Pub/Sub Ordering:</strong> N√£o garante ordem por padr√£o - use ordering keys se necess√°rio</li>
                        <li><strong>Storage Classes:</strong> Nearline = acesso 1x/m√™s, Coldline = 1x/quarter, Archive = 1x/ano</li>
                        <li><strong>IAM Bindings:</strong> Roles s√£o herdados da hierarquia (Org ‚Üí Folder ‚Üí Project ‚Üí Resource)</li>
                        <li><strong>Case Studies:</strong> Memorize os 3-4 case studies - perguntas referenciam eles diretamente</li>
                    </ul>
                </section>

                <section>
                    <h2>üéØ Checklist Final</h2>
                    <ul>
                        <li>‚úÖ Domino BigQuery (partitioning, clustering, ML)</li>
                        <li>‚úÖ Sei quando usar Dataflow vs Dataproc vs Cloud Functions</li>
                        <li>‚úÖ Conhe√ßo Pub/Sub e seus use cases</li>
                        <li>‚úÖ Entendo storage classes e lifecycle policies</li>
                        <li>‚úÖ Sei configurar IAM roles e permissions</li>
                        <li>‚úÖ Conhe√ßo Vertex AI e BigQuery ML</li>
                        <li>‚úÖ Li e entendi os 3-4 case studies oficiais</li>
                        <li>‚úÖ Fiz 10+ labs pr√°ticos no GCP</li>
                        <li>‚úÖ Completei 3+ practice exams com 75%+ score</li>
                    </ul>
                </section>

                <div class="navigation-buttons">
                    <a href="../index.html#certifications" class="btn btn-secondary">
                        <i class="fas fa-arrow-left"></i> Voltar
                    </a>
                    <a href="https://cloud.google.com/learn/certification/data-engineer" class="btn btn-primary" target="_blank">
                        Agendar Exame <i class="fas fa-external-link-alt"></i>
                    </a>
                </div>
            </div>
        </div>
    </section>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2024 Engenharia de Dados Roadmap. Open Source Project.</p>
        </div>
    </footer>

    <style>
        .cert-detail {
            padding: 6rem 0 4rem;
            background: var(--light);
        }

        .cert-header {
            text-align: center;
            margin-bottom: 3rem;
        }

        .cert-provider.gcp {
            display: inline-block;
            padding: 0.75rem 1.5rem;
            border-radius: 0.5rem;
            background: linear-gradient(135deg, #4285f4 0%, #34a853 50%, #fbbc05 75%, #ea4335 100%);
            color: white;
            font-weight: 700;
            font-size: 1.25rem;
            margin-bottom: 1rem;
        }

        .cert-header h1 {
            font-size: 3rem;
            margin-bottom: 1rem;
        }

        .cert-meta {
            display: flex;
            justify-content: center;
            gap: 2rem;
            flex-wrap: wrap;
            color: var(--text-light);
        }

        .cert-content section {
            background: white;
            padding: 2rem;
            border-radius: 1rem;
            margin-bottom: 2rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.05);
        }

        .domains-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin-top: 2rem;
        }

        .domain-card {
            border: 2px solid var(--border);
            border-radius: 0.75rem;
            overflow: hidden;
        }

        .domain-header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 1rem;
        }

        .domain-header h3 {
            margin: 0;
            font-size: 1rem;
        }

        .domain-card ul {
            padding: 1rem;
            margin: 0;
        }

        .study-plan {
            margin-top: 2rem;
        }

        .week {
            border-left: 4px solid var(--primary);
            padding-left: 1.5rem;
            margin-bottom: 2rem;
        }

        .week h3 {
            color: var(--primary);
            margin-top: 0;
        }

        .component-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin-top: 2rem;
        }

        .component-card {
            border: 2px solid var(--border);
            border-radius: 0.75rem;
            padding: 1.5rem;
        }

        .component-card h3 {
            color: var(--primary);
            margin-top: 0;
        }

        .resources-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1.5rem;
            margin-top: 2rem;
        }

        .resource-card {
            border: 2px solid var(--border);
            border-radius: 0.75rem;
            padding: 1.5rem;
        }

        .resource-card h3 {
            margin-top: 0;
            color: var(--primary);
        }

        .navigation-buttons {
            display: flex;
            justify-content: space-between;
            gap: 1rem;
            margin-top: 3rem;
        }

        pre code {
            display: block;
            padding: 1.5rem;
            background: #1e1e1e;
            color: #d4d4d4;
            border-radius: 0.5rem;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 0.9rem;
            line-height: 1.5;
        }

        @media (max-width: 768px) {
            .cert-header h1 {
                font-size: 2rem;
            }

            .navigation-buttons {
                flex-direction: column;
            }

            .domain-header h3 {
                font-size: 0.9rem;
            }
        }
    </style>
</body>
</html>
